{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Creating a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for layers in the network\n",
    "    \n",
    "    Arguments:\n",
    "        `inbound_layers`: A list of layers with edges into this class\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_layers = []):\n",
    "        \n",
    "        # The list of layers with edges into the class\n",
    "        self.inbound_layers = inbound_layers\n",
    "        \n",
    "        # The value of this layer which is calculated during the forward pass\n",
    "        self.value = None\n",
    "        \n",
    "        # The layers that the this layer outputs to\n",
    "        self.outbound_layers = []\n",
    "        \n",
    "        # The gradients for this layer\n",
    "        # The keys are the input to this layer and their values are the \n",
    "        # partials of this layer with respect to that layer \n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Sets this layer as an outbound layer for all of this layer's inputs\n",
    "        for layer in inbound_layers: \n",
    "            layer.outbound_layers.append(self)\n",
    "        \n",
    "    def forward(debug = False):\n",
    "        # Abstract method that should be implemented for all the derived classes\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward():\n",
    "        # Abstract method that should be implemented for all the derived classes \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    This layer accepts inputs to the neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Note here that nothing is set because these values are set during\n",
    "        # the topological sort\n",
    "        Layer.__init__(self)\n",
    "        \n",
    "    def forward(self, debug = False):\n",
    "        # Do nothing because nothing is calculated\n",
    "        pass\n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "        # An Input layer has no inputs so the gradient is zero \n",
    "        self.gradients = {self : 0}\n",
    "        \n",
    "        # Weights and bias may be inputs, so we need to sum the gradients \n",
    "        # from their outbound layers during the backward pass.\n",
    "        \n",
    "        # Remember that the goal is to figure out the total change in the cost function\n",
    "        # with respect to a single parameter, hence the addition\n",
    "  \n",
    "        for n in self.outbound_layers:\n",
    "#             a = self.gradients[self]\n",
    "#             print(a)\n",
    "#             b = n.gradients[self]\n",
    "#             print(b)\n",
    "            self.gradients[self] += n.gradients[self] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, X, W, b):\n",
    "        Layer.__init__(self, [X, W, b])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        X = self.inbound_layers[0].value\n",
    "        W = self.inbound_layers[1].value\n",
    "        b = self.inbound_layers[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "       \n",
    "        if debug:\n",
    "            print(\"Input to layer is:\")\n",
    "            print(X)\n",
    "\n",
    "            print(\"Weights of layer is:\")\n",
    "            print(W)\n",
    "\n",
    "            print(\"Bias of layer is:\")\n",
    "            print(b)\n",
    "            \n",
    "            print(\"XW + b is:\")\n",
    "            print(self.value)\n",
    "            \n",
    "    def backward(self, debug = False):\n",
    "        \n",
    "        # Initialize a partial derivative for each of the inbound_layers,\n",
    "        # remembering here that this dictionary stores the partial derivative of\n",
    "        # this layer with respect to the inbound layers\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        \n",
    "        for n in self.outbound_layers:\n",
    "            # Get the partial derivative for each of the variables in this layer \n",
    "            # with respect to the cost\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            if debug:\n",
    "                print(\"grad_cost is:\") \n",
    "                print(grad_cost)\n",
    "                \n",
    "            \n",
    "            \n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, self.inbound_layers[1].value.T) \n",
    "           \n",
    "            self.gradients[self.inbound_layers[1]] += np.dot(self.inbound_layers[0].value.T, grad_cost)\n",
    "              \n",
    "            self.gradients[self.inbound_layers[2]] += np.sum(grad_cost, axis = 0, keepdims = False)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"The derivatives of the cost with respect to the inputs are:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "            \n",
    "            print(\"The derivatives of the cost with respect to the weights are:\")\n",
    "            print(self.gradients[self.inbound_layers[1]])\n",
    "            \n",
    "            print(\"The derivatives of the cost with respect to the biases are:\")\n",
    "            print(self.gradients[self.inbound_layers[2]])\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self, layer):\n",
    "        Layer.__init__(self, [layer])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1. + np.exp(-x))\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        \n",
    "        self.value = self._sigmoid(self.inbound_layers[0].value)\n",
    "            \n",
    "        if debug:\n",
    "            print(\"Input to sigmoid layer is:\")\n",
    "            print(self.inbound_layers[0].value)\n",
    "            \n",
    "            print(\"Value after sigmoid activation is:\")\n",
    "            print(self.value)\n",
    "            \n",
    "        \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients = {n : np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        \n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_layers[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "        \n",
    "        if debug:\n",
    "            print(\"The derivatives of the cost with respect to the sigmoid activation is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MSE(Layer):\n",
    "    def __init__(self, y, a):\n",
    "        Layer.__init__(self, [y, a])\n",
    "        \n",
    "    def forward(self, debug = False):\n",
    "        y = self.inbound_layers[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_layers[1].value.reshape(-1, 1) \n",
    "    \n",
    "        # get the number of samples\n",
    "        self.m = self.inbound_layers[0].value.shape[0] \n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    " \n",
    "        if debug:\n",
    "            print(\"True value of y is:\")\n",
    "            print(y)\n",
    "\n",
    "            print(\"Predicted value of y is:\")\n",
    "            print(a)\n",
    "             \n",
    "            print(\"y - a is:\")\n",
    "            print(self.diff)\n",
    "\n",
    "       \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients[self.inbound_layers[0]] = (2/self.m) * self.diff\n",
    "        self.gradients[self.inbound_layers[1]] = (-2/self.m) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self, logits):\n",
    "        Layer.__init__(self, [logits])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        \n",
    "        exp_logits = np.exp(self.inbound_layers[0].value)\n",
    "        sum_exp = np.sum(exp_logits)  \n",
    "        self.value = exp_logits/sum_exp\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Logits are:\")\n",
    "            print(self.inbound_layers[0].value) \n",
    "            \n",
    "            print(\"After exponents are:\")\n",
    "            exp_logits = np.exp(self.inbound_layers[0].value)\n",
    "            print(exp_logits)\n",
    "            \n",
    "            print(\"Probabilities are:\")\n",
    "            sum_exp = np.sum(exp_logits)  \n",
    "            self.value = exp_logits/sum_exp \n",
    "            print(self.value)\n",
    "             \n",
    "            \n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "         \n",
    "        # Define gradient for inbound layers\n",
    "        self.gradients = {n : np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        jacobian = self._calc_jacobian(self.value)\n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            if debug:\n",
    "                print(\"grad_cost is:\")\n",
    "                print(grad_cost)\n",
    "                print(\"The Jacobian is:\")\n",
    "                print(jacobian)\n",
    "            \n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, jacobian)\n",
    "        \n",
    " \n",
    "        if debug: \n",
    "            print(\"The derivative of the cost with respect to the inputs of the softmax layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "       \n",
    "    def _calc_jacobian(self, probs):\n",
    "        \n",
    "        # First calculate the off diagonal derivatives\n",
    "        jacobian = np.dot(-1 * probs.T, probs)\n",
    "        dims = jacobian.shape[0]\n",
    "        \n",
    "        # Now calculate the diagonal derivatives\n",
    "        for i in range(dims):\n",
    "            jacobian[i,i] = probs[0,i] * (1 - probs[0,i])\n",
    "        return(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Layer):\n",
    "    def __init__(self, y, probs):\n",
    "        Layer.__init__(self, [y, probs])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        n_samples_in_batch = self.inbound_layers[0].value.shape[0]\n",
    "        n_classes = self.inbound_layers[0].value.shape[1]\n",
    "        \n",
    "        self.y_flat = self.inbound_layers[0].value.reshape(n_samples_in_batch, n_classes)\n",
    "        self.probs_flat = self.inbound_layers[1].value.reshape(n_samples_in_batch, n_classes)\n",
    "       \n",
    "        # Calculate the accuracy\n",
    "        n_correct = np.sum(np.argmax(self.y_flat, axis = 1) == np.argmax(self.probs_flat, axis = 1))\n",
    "        self.accuracy = n_correct/n_samples_in_batch\n",
    "        \n",
    "        # Calculate the cross entropy\n",
    "        self.log_probs = np.log(self.probs_flat)\n",
    "        self.cross_entropy = self.y_flat * self.log_probs\n",
    "        self.value = -1 * np.sum(self.cross_entropy)\n",
    "       \n",
    "        \n",
    "        if debug:\n",
    "            print(\"True values y are:\")\n",
    "            print(self.y_flat)\n",
    "            print(\"Probabilities are:\")\n",
    "            print(self.probs_flat)\n",
    "            print(\"True value y max index are:\")\n",
    "            print(np.argmax(self.y_flat, axis = 1))\n",
    "            print(np.argmax(self.y_flat, axis = 1).shape)\n",
    "            print(\"Probabilities max index are:\")\n",
    "            print(np.argmax(self.probs_flat, axis = 1))\n",
    "            print(np.argmax(self.probs_flat, axis = 1).shape) \n",
    "            print(\"Log probabilities are:\")\n",
    "            print(self.log_probs)\n",
    "            print(\"Cross entropy is:\")\n",
    "            print(self.cross_entropy)\n",
    "            print(\"Cross entropy sum is\")\n",
    "            print(self.value)\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients[self.inbound_layers[0]] = -1 * 1/self.probs_flat\n",
    "        self.gradients[self.inbound_layers[1]] = -1 * self.y_flat/self.probs_flat\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Gradients of cross entropy with respect to y layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "            print(\"Gradients of cross entropy with respect to softmax layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "    \n",
    "    G = {}\n",
    "    \n",
    "    layers = [n for n in input_layers]\n",
    "    \n",
    "    # Think of each element in the layer as a node, in this while loop\n",
    "    # we are simply finding which layers are connected to which other layers\n",
    "    while len(layers) > 0:\n",
    "        # Get the first element of the array\n",
    "        n = layers.pop(0)\n",
    "        \n",
    "        # Check if this layer n is in the dictionary if it isn't add it in\n",
    "        if n not in G:\n",
    "            G[n] = {'in' : set(), 'out' : set()}\n",
    "        # Check if this layer m is in the dictionary if it isn't add it in \n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in' : set(), 'out' : set()}\n",
    "            # Add the edges between the nodes\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "        \n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    \n",
    "    while len(S) > 0:\n",
    "        # Get the last layer \n",
    "        n = S.pop()\n",
    "        \n",
    "        # Check if it is an input layer, if it is then initialize the value\n",
    "        if (isinstance(n, Input)):\n",
    "            n.value = feed_dict[n]\n",
    "            \n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            \n",
    "            # if there are no incoming edges to m then add it to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(graph, debug = False):\n",
    "    for n in graph:\n",
    "        n.forward(debug)\n",
    "\n",
    "def backward_pass(graph, debug = False):\n",
    "    for n in graph[::-1]:\n",
    "        n.backward(debug) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_update(trainable, learning_rate = 1e-2): \n",
    "     \n",
    "    for t in trainable:\n",
    "        partial = t.gradients[t]\n",
    "#         print(\"Partial derivatives are:\")\n",
    "#         print(partial)\n",
    "        t.value -= learning_rate * partial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network to predict Boston housing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# Load the Boston housing data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "print(X_.shape)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323, 13)\n",
      "(81, 13)\n",
      "(323,)\n",
      "(81,)\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data so that the SGD will perform better\n",
    "X_ = (X_-np.mean(X_,axis = 0))/np.std(X_, axis = 0) # note here that axis = 0 is the vertical axis\n",
    "# print(X_)\n",
    "# Now split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size = 0.2)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup the hidden layer\n",
    "n_features = X_train.shape[1]\n",
    "n_hidden = 10\n",
    "\n",
    "# Initialize the weights \n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Build the layers for the neural network\n",
    "X, y, = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "# Define the input layers to the neural network \n",
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    y: y_train,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup the parameters for training the network\n",
    "epochs = 500\n",
    "show_per_step = 50\n",
    "n_samples = X_train.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = n_samples // batch_size\n",
    "\n",
    "# Now define the graph\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_pass(graph)\n",
    "trainables = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of validation set in epoch 1 is: 330.55201113894464\n",
      "Loss of validation set in epoch 51 is: 31.30314767263208\n",
      "Loss of validation set in epoch 101 is: 22.270549038667728\n",
      "Loss of validation set in epoch 151 is: 17.908483254914582\n",
      "Loss of validation set in epoch 201 is: 15.390397758119743\n",
      "Loss of validation set in epoch 251 is: 14.770775973639338\n",
      "Loss of validation set in epoch 301 is: 14.03109198395931\n",
      "Loss of validation set in epoch 351 is: 13.924274466827992\n",
      "Loss of validation set in epoch 401 is: 13.714337412500448\n",
      "Loss of validation set in epoch 451 is: 13.914879352902435\n",
      "Loss of test set is: 12.904602117981813\n"
     ]
    }
   ],
   "source": [
    "# Now lets run the model\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Sample a random batch of data \n",
    "        X_batch, y_batch = resample(X_train, y_train, n_samples = batch_size)\n",
    "        \n",
    "        # Reset the values of X and y \n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "#         print(X.value)\n",
    "#         print(y.value)\n",
    "        \n",
    "        # Now run the forward and backward propagation\n",
    "        forward_pass(graph) \n",
    "        backward_pass(graph)\n",
    "        \n",
    "        # Update the weights of or biases and weights\n",
    "        sgd_update(trainables, learning_rate = 1e-3) \n",
    "        \n",
    "#         print(\"Loss is {0}\".format(graph[-1].value))\n",
    "        loss += graph[-1].value\n",
    "#     print(\"Epoch: {}, Loss {:.3f}\".format(i + 1, loss/steps_per_epoch))\n",
    "    \n",
    "    # Use the validation set\n",
    "    X.value = X_validation\n",
    "    y.value = y_validation\n",
    "    \n",
    "    forward_pass(graph)\n",
    "    \n",
    "    if (i%show_per_step == 0):\n",
    "        print(\"Loss of validation set in epoch {0} is: {1}\".format(i + 1, graph[-1].value))\n",
    "\n",
    "# Test it on the test set\n",
    "X.value = X_test\n",
    "y.value = y_test\n",
    "forward_pass(graph)\n",
    "print(\"Loss of test set is: {0}\".format(graph[-1].value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network to classify numbers from the NMIST data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just using keras to import the data\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Use one hot encoding for the y label vector\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "y_train_one_hot = lb.transform(y_train)\n",
    "y_test_one_hot = lb.transform(y_test)\n",
    "print(y_train_one_hot.shape)\n",
    "print(y_test_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b12e668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Print the first image from the training set out\n",
    "X_batch, y_batch = resample(X_train, y_train_one_hot, n_samples = 1)\n",
    "plt.imshow(X_batch[0], cmap = plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the grayscale image so that the values range between -0.5 and 0.5, this is so that the sigmoid activation \n",
    "# function does not saturate during training \n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [-0.5, 0.5]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    img_min = np.min(image_data)\n",
    "    img_max = np.max(image_data)\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    scaled_img = a + ((image_data - img_min) * (b-a))/(img_max - img_min)\n",
    "    return(scaled_img)\n",
    "\n",
    "X_train_normalize = normalize_grayscale(X_train)\n",
    "X_test_normalize = normalize_grayscale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      "  -0.42941176 -0.42941176 -0.42941176 -0.00588235  0.03333333  0.18627451\n",
      "  -0.39803922  0.15098039  0.5         0.46862745 -0.00196078 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.38235294 -0.35882353 -0.13137255  0.10392157  0.16666667\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.49215686  0.38235294\n",
      "   0.1745098   0.49215686  0.44901961  0.26470588 -0.24901961 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.30784314  0.43333333  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.48431373 -0.13529412\n",
      "  -0.17843137 -0.17843137 -0.28039216 -0.34705882 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.42941176  0.35882353  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.27647059  0.21372549  0.46862745  0.44509804 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.18627451  0.11176471 -0.08039216  0.49215686  0.49215686\n",
      "   0.30392157 -0.45686275 -0.5        -0.33137255  0.10392157 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.44509804 -0.49607843  0.10392157  0.49215686\n",
      "  -0.14705882 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5         0.04509804  0.49215686\n",
      "   0.24509804 -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.45686275  0.24509804\n",
      "   0.49215686 -0.2254902  -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.3627451\n",
      "   0.44509804  0.38235294  0.12745098 -0.07647059 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.18235294  0.44117647  0.49215686  0.49215686 -0.03333333 -0.40196078\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.32352941  0.22941176  0.49215686  0.49215686  0.08823529 -0.39411765\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.4372549  -0.13529412  0.48823529  0.49215686  0.23333333\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5         0.47647059  0.49215686  0.47647059\n",
      "  -0.24901961 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.31960784  0.00980392  0.21764706  0.49215686  0.49215686  0.31176471\n",
      "  -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.34705882\n",
      "   0.08039216  0.39803922  0.49215686  0.49215686  0.49215686  0.48039216\n",
      "   0.21372549 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.40588235 -0.05294118  0.36666667\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.28823529 -0.19411765\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.40980392 -0.24117647  0.33529412  0.49215686  0.49215686\n",
      "   0.49215686  0.49215686  0.27647059 -0.18235294 -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.42941176  0.17058824  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.26470588 -0.18627451 -0.46470588 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.28431373  0.1745098\n",
      "   0.38627451  0.49215686  0.49215686  0.49215686  0.49215686  0.45686275\n",
      "   0.02156863 -0.45686275 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5         0.03333333  0.49215686\n",
      "   0.49215686  0.49215686  0.33137255  0.02941176  0.01764706 -0.4372549\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the data and before and after the reshape to see if we did it properly \n",
    "print(X_train[0])\n",
    "print(X_train_normalize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now flatten the array into a 1 dimension array\n",
    "X_train_normalize = np.array(X_train_normalize).reshape(60000, 28 * 28)\n",
    "X_test_normalize = np.array(X_test_normalize).reshape(10000, 28 * 28 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      " -0.42941176 -0.42941176 -0.42941176 -0.00588235  0.03333333  0.18627451\n",
      " -0.39803922  0.15098039  0.5         0.46862745 -0.00196078 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.38235294 -0.35882353 -0.13137255\n",
      "  0.10392157  0.16666667  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.38235294  0.1745098   0.49215686  0.44901961  0.26470588\n",
      " -0.24901961 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.30784314\n",
      "  0.43333333  0.49215686  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.49215686  0.48431373 -0.13529412 -0.17843137\n",
      " -0.17843137 -0.28039216 -0.34705882 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.42941176  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.27647059  0.21372549  0.46862745  0.44509804\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.18627451  0.11176471\n",
      " -0.08039216  0.49215686  0.49215686  0.30392157 -0.45686275 -0.5\n",
      " -0.33137255  0.10392157 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.44509804 -0.49607843  0.10392157  0.49215686 -0.14705882 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5         0.04509804  0.49215686  0.24509804 -0.49215686\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.45686275  0.24509804  0.49215686\n",
      " -0.2254902  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.3627451\n",
      "  0.44509804  0.38235294  0.12745098 -0.07647059 -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.18235294  0.44117647  0.49215686  0.49215686 -0.03333333 -0.40196078\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.32352941  0.22941176  0.49215686  0.49215686\n",
      "  0.08823529 -0.39411765 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.4372549  -0.13529412\n",
      "  0.48823529  0.49215686  0.23333333 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  0.47647059  0.49215686  0.47647059 -0.24901961 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.31960784  0.00980392\n",
      "  0.21764706  0.49215686  0.49215686  0.31176471 -0.49215686 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.34705882  0.08039216  0.39803922\n",
      "  0.49215686  0.49215686  0.49215686  0.48039216  0.21372549 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.40588235 -0.05294118  0.36666667  0.49215686\n",
      "  0.49215686  0.49215686  0.49215686  0.28823529 -0.19411765 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.40980392 -0.24117647  0.33529412  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.27647059 -0.18235294 -0.49215686 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.42941176  0.17058824  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.26470588 -0.18627451 -0.46470588 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.28431373  0.1745098   0.38627451  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.45686275  0.02156863 -0.45686275 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5         0.03333333  0.49215686  0.49215686  0.49215686  0.33137255\n",
      "  0.02941176  0.01764706 -0.4372549  -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5       ]\n"
     ]
    }
   ],
   "source": [
    "# See if we did the reshaping correctly\n",
    "print(X_train_normalize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Now define a validation set\n",
    "X_train_normalize, X_validation_normalize, y_train_one_hot, y_validation_one_hot = train_test_split(X_train_normalize, y_train_one_hot, test_size = 0.2)\n",
    "\n",
    "# Check that the shape is correct\n",
    "print(X_train_normalize.shape)\n",
    "print(y_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets define the network again!\n",
    "n_hidden = 30\n",
    "n_hidden_2 = 15\n",
    "n_classes = 10\n",
    "n_features = X_train_normalize.shape[1]\n",
    "\n",
    "# Initialize the weights \n",
    "W1_ = np.random.normal(loc = 0, scale = 1, size = (n_features, n_hidden))\n",
    "b1_ = np.zeros(n_hidden)\n",
    "# W2_ = np.random.normal(loc = 0, scale = 0.1, size = (n_hidden, n_hidden_2))\n",
    "# b2_ = np.zeros(n_hidden_2)\n",
    "# W3_ = np.random.normal(loc = 0, scale = 0.1, size = (n_hidden_2, n_classes))\n",
    "# b3_ = np.zeros(n_classes)\n",
    "\n",
    "W2_ = np.random.normal(loc = 0, scale = 1, size = (n_hidden, n_classes))\n",
    "b2_ = np.zeros(n_classes)\n",
    "# Build the layers for the neural network\n",
    "X, y, = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "# W3, b3 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "# s2 = Sigmoid(l2)\n",
    "# l3 = Linear(s2, W3, b3)\n",
    "probs = Softmax(l2)\n",
    "cost = CrossEntropy(y, probs)\n",
    "\n",
    "# Define the input layers to the neural network \n",
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    y: y_train_one_hot,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "#     W3: W3_,\n",
    "#     b3: b3_\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 784)\n",
      "(2, 10)\n",
      "Input to layer is:\n",
      "[[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "Weights of layer is:\n",
      "[[ 0.26185247 -1.15685857  0.24583694 ..., -0.94514361 -0.86012973\n",
      "  -0.18252562]\n",
      " [-0.20510036  0.53140258 -0.14920534 ...,  0.87965589  0.42067083\n",
      "   0.64409343]\n",
      " [ 0.55892088  1.63185814  0.82323487 ...,  0.10871072 -0.89987598\n",
      "  -0.45747789]\n",
      " ..., \n",
      " [-0.431204    1.09761647  0.78477457 ...,  0.86980691 -0.63449387\n",
      "   0.5054002 ]\n",
      " [ 0.78253423  0.18017127 -1.41534909 ..., -1.70062048  1.86120187\n",
      "  -0.81102177]\n",
      " [-0.58976733 -0.50319713 -1.34858261 ..., -1.89592297  1.62104425\n",
      "   0.36754219]]\n",
      "Bias of layer is:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "XW + b is:\n",
      "[[  7.18353909  -5.85984301  17.29693059   5.68348086  -2.28590781\n",
      "  -12.41860408  14.86389333  -1.99881055 -34.5771487   -5.93714544\n",
      "    7.38980397  -0.89169152  -6.0697439   19.96829208 -11.96773061\n",
      "   22.29645841   8.47311385 -24.68841556  -2.15992366   0.12047379\n",
      "    6.43507448  17.29189972  15.18709323 -25.12222518  12.90542686\n",
      "   12.61592199   6.5935979    2.22195063   0.04516917 -10.45479213]\n",
      " [ 10.78611631 -12.48419873  17.80628351  16.05433465  -1.6477891\n",
      "   -4.59967324  14.51079466  -6.45432639  -9.20402037 -12.46159737\n",
      "    2.65688218  -4.0729401    3.60263557  21.2926365  -10.3028816\n",
      "   16.0213948    6.17947955 -15.04404494 -11.95088635  12.68703932\n",
      "    3.6826556   -1.73179098  12.18120673 -24.51560035  22.89545299\n",
      "   16.33300146  17.86824981   3.06519318  -6.01236907  -6.76666876]]\n",
      "Input to sigmoid layer is:\n",
      "[[  7.18353909  -5.85984301  17.29693059   5.68348086  -2.28590781\n",
      "  -12.41860408  14.86389333  -1.99881055 -34.5771487   -5.93714544\n",
      "    7.38980397  -0.89169152  -6.0697439   19.96829208 -11.96773061\n",
      "   22.29645841   8.47311385 -24.68841556  -2.15992366   0.12047379\n",
      "    6.43507448  17.29189972  15.18709323 -25.12222518  12.90542686\n",
      "   12.61592199   6.5935979    2.22195063   0.04516917 -10.45479213]\n",
      " [ 10.78611631 -12.48419873  17.80628351  16.05433465  -1.6477891\n",
      "   -4.59967324  14.51079466  -6.45432639  -9.20402037 -12.46159737\n",
      "    2.65688218  -4.0729401    3.60263557  21.2926365  -10.3028816\n",
      "   16.0213948    6.17947955 -15.04404494 -11.95088635  12.68703932\n",
      "    3.6826556   -1.73179098  12.18120673 -24.51560035  22.89545299\n",
      "   16.33300146  17.86824981   3.06519318  -6.01236907  -6.76666876]]\n",
      "Value after sigmoid activation is:\n",
      "[[  9.99241599e-01   2.84358232e-03   9.99999969e-01   9.96609835e-01\n",
      "    9.22968144e-02   4.04265816e-06   9.99999649e-01   1.19327863e-01\n",
      "    9.62354582e-16   2.63260477e-03   9.99382864e-01   2.90760880e-01\n",
      "    2.30643325e-03   9.99999998e-01   6.34567580e-06   1.00000000e+00\n",
      "    9.99791031e-01   1.89651969e-11   1.03407529e-01   5.30082072e-01\n",
      "    9.98398280e-01   9.99999969e-01   9.99999746e-01   1.22901231e-11\n",
      "    9.99997515e-01   9.99996681e-01   9.98632767e-01   9.02203440e-01\n",
      "    5.11290372e-01   2.88090512e-05]\n",
      " [  9.99979316e-01   3.78599238e-06   9.99999982e-01   9.99999893e-01\n",
      "    1.61407983e-01   9.95502186e-03   9.99999501e-01   1.57122634e-03\n",
      "    1.00623875e-04   3.87253495e-06   9.34433905e-01   1.67421797e-02\n",
      "    9.73471155e-01   9.99999999e-01   3.35351928e-05   9.99999890e-01\n",
      "    9.97932777e-01   2.92721197e-07   6.45346857e-06   9.99996909e-01\n",
      "    9.75461218e-01   1.50358638e-01   9.99994874e-01   2.25429137e-11\n",
      "    1.00000000e+00   9.99999919e-01   9.99999983e-01   9.55433946e-01\n",
      "    2.44230171e-03   1.15019979e-03]]\n",
      "Input to layer is:\n",
      "[[  9.99241599e-01   2.84358232e-03   9.99999969e-01   9.96609835e-01\n",
      "    9.22968144e-02   4.04265816e-06   9.99999649e-01   1.19327863e-01\n",
      "    9.62354582e-16   2.63260477e-03   9.99382864e-01   2.90760880e-01\n",
      "    2.30643325e-03   9.99999998e-01   6.34567580e-06   1.00000000e+00\n",
      "    9.99791031e-01   1.89651969e-11   1.03407529e-01   5.30082072e-01\n",
      "    9.98398280e-01   9.99999969e-01   9.99999746e-01   1.22901231e-11\n",
      "    9.99997515e-01   9.99996681e-01   9.98632767e-01   9.02203440e-01\n",
      "    5.11290372e-01   2.88090512e-05]\n",
      " [  9.99979316e-01   3.78599238e-06   9.99999982e-01   9.99999893e-01\n",
      "    1.61407983e-01   9.95502186e-03   9.99999501e-01   1.57122634e-03\n",
      "    1.00623875e-04   3.87253495e-06   9.34433905e-01   1.67421797e-02\n",
      "    9.73471155e-01   9.99999999e-01   3.35351928e-05   9.99999890e-01\n",
      "    9.97932777e-01   2.92721197e-07   6.45346857e-06   9.99996909e-01\n",
      "    9.75461218e-01   1.50358638e-01   9.99994874e-01   2.25429137e-11\n",
      "    1.00000000e+00   9.99999919e-01   9.99999983e-01   9.55433946e-01\n",
      "    2.44230171e-03   1.15019979e-03]]\n",
      "Weights of layer is:\n",
      "[[  1.36500924e+00   1.10062103e+00   5.37334033e-01   5.65539768e-02\n",
      "   -9.27510564e-01  -5.88495744e-01   1.95246584e-01  -7.25234963e-01\n",
      "    2.39118302e-01   2.33875580e+00]\n",
      " [  2.87029460e-01  -1.44753048e+00   3.80012504e-02   2.80474683e-01\n",
      "    3.95185202e-01  -1.17774963e-01   6.25596531e-01  -7.72209582e-01\n",
      "   -3.20253271e-01  -1.06472057e+00]\n",
      " [  2.03527084e-01  -8.78550295e-01  -2.05825009e+00   8.66188890e-01\n",
      "   -1.23425330e-01  -1.44645229e+00  -2.08433048e-01  -7.66501858e-01\n",
      "    3.91703541e-01   1.09526980e+00]\n",
      " [ -2.25144914e-02   7.41249668e-01  -7.30024346e-01  -2.04426724e+00\n",
      "    2.48258280e-01   3.45838142e-02  -5.42862232e-01  -1.89637078e-01\n",
      "    5.06547805e-01   6.87278871e-01]\n",
      " [ -1.24258907e+00  -6.41735730e-02  -9.81007069e-01   6.78525554e-01\n",
      "    2.73831741e+00   6.46962761e-01   1.41514997e-01   6.22301591e-01\n",
      "   -6.07885450e-02  -1.04842503e-01]\n",
      " [  8.07788309e-01   2.35534063e-01  -2.64818165e-01   5.75318589e-01\n",
      "   -2.50112593e-01  -1.15130708e+00  -1.28511295e+00  -1.67206922e+00\n",
      "   -9.46774879e-04   2.73429751e-02]\n",
      " [  2.93156161e-01  -6.62882181e-01  -1.47951413e+00  -2.24073115e+00\n",
      "    2.40323405e+00   2.10343435e+00   1.01287746e+00   2.52782469e-01\n",
      "    8.26195173e-02  -3.92196004e-01]\n",
      " [ -1.69775981e+00   3.88507063e-01  -1.02599276e+00  -4.44337892e-02\n",
      "    1.31918413e+00   1.28075674e+00   1.90955912e+00  -1.56748883e+00\n",
      "   -1.05737040e-01  -3.38050505e-01]\n",
      " [  4.00496676e-01   1.36140113e+00   2.06869278e+00   1.32760087e+00\n",
      "    4.37775506e-01   5.53403196e-01   6.78352856e-01  -6.93619183e-01\n",
      "   -5.01589004e-01   3.33988003e-01]\n",
      " [ -5.87902362e-01  -4.12994172e-01  -2.55832899e-01  -6.62943504e-01\n",
      "   -6.44587703e-02  -1.10057751e+00   2.01927266e+00  -1.88092510e+00\n",
      "    9.11028066e-01   1.71980344e-01]\n",
      " [ -1.48710819e-01  -7.55882300e-01  -1.94831076e+00  -2.45542832e+00\n",
      "   -6.77645279e-01  -9.88476718e-01  -4.64243949e-01   4.43093729e-01\n",
      "   -4.37240078e-01  -1.24273300e+00]\n",
      " [  1.20233715e+00   1.37135904e-01   3.81537948e-01   5.32183721e-02\n",
      "   -1.66683686e+00  -9.44850402e-01   9.31849400e-01   2.04898812e-01\n",
      "    1.65851901e-01  -1.15759423e+00]\n",
      " [ -1.32304014e+00  -8.32890762e-01   1.01468980e+00   6.95745064e-01\n",
      "   -1.93890350e-01   8.77119814e-02  -7.77186824e-02  -1.19552533e-01\n",
      "    1.90114292e+00   1.57615915e-01]\n",
      " [  4.07690168e-01   3.41026213e-01   1.05242524e+00  -2.72266035e-02\n",
      "    2.25864263e+00   5.13305724e-01  -7.18805375e-01   2.12597696e+00\n",
      "    2.85121826e-01   1.56325083e-01]\n",
      " [ -1.54142492e+00  -4.96685653e-01   1.40550452e+00  -1.40884184e+00\n",
      "   -2.86546636e-01   1.35446371e+00  -2.84651485e+00  -1.37689896e+00\n",
      "    2.16152581e-01  -8.21367217e-01]\n",
      " [ -6.07947962e-02   6.74059579e-01   3.15206199e-03  -1.21034927e+00\n",
      "   -3.37755540e-01  -8.19635712e-01   8.31503018e-01  -1.02864676e+00\n",
      "   -8.22295006e-01   9.45859379e-01]\n",
      " [ -9.81314448e-01   2.09036203e-02  -3.26772930e-01  -1.92902865e-01\n",
      "   -3.49653870e-01   1.79779425e-01  -8.99177771e-02   2.15680507e-02\n",
      "   -3.49144115e-01  -7.97718580e-01]\n",
      " [  1.62960494e+00  -5.12362598e-01   1.44046662e-01   6.96903804e-01\n",
      "   -2.27003909e+00   2.06399927e+00   4.51381578e-01  -9.30835136e-01\n",
      "   -1.97189380e-01  -9.37707882e-01]\n",
      " [ -1.68307658e+00   1.57522372e-01  -1.03379744e-01   4.12836405e-01\n",
      "   -4.79825939e-01  -1.64423400e+00   4.48580529e-01  -3.08654430e-01\n",
      "   -1.33043097e+00  -1.39757700e+00]\n",
      " [  3.50562917e-01  -9.46724171e-02  -1.16525341e+00   1.54680281e-01\n",
      "    9.52730444e-01   3.33798003e-01   9.49197891e-01   2.30066111e-01\n",
      "   -2.39803366e-01  -6.53665369e-02]\n",
      " [ -1.20542070e+00   1.17316175e-01  -2.98217586e-01  -3.71670281e-01\n",
      "    1.63966152e-01   7.06387929e-01  -1.16462119e+00   7.46075270e-01\n",
      "    3.38815628e-01  -5.22511009e-01]\n",
      " [ -5.36438591e-01  -4.25595521e-01  -6.66386792e-01  -3.90879143e-01\n",
      "    4.65712563e-01   1.12761343e+00  -9.04302735e-01  -1.11801700e+00\n",
      "   -1.81339768e-01   1.05465712e+00]\n",
      " [  2.22266626e+00   5.16751283e-01  -6.14258372e-01   1.16807416e+00\n",
      "   -3.00825011e-01  -5.50019958e-01   6.94242809e-01   8.91633134e-01\n",
      "   -6.63973591e-01  -1.28901728e+00]\n",
      " [  3.60165194e-02  -7.39984925e-02  -1.43437848e-01   1.64463175e+00\n",
      "   -1.25386821e-01   2.13905124e-01   8.18356376e-02   3.55544383e-01\n",
      "    1.24658071e+00  -1.50280774e+00]\n",
      " [ -5.47646558e-01  -6.65909877e-01  -2.45270318e+00   1.15975458e+00\n",
      "    8.59979131e-01  -9.33710173e-01  -3.54688234e-01   2.56454429e+00\n",
      "   -2.20725711e+00  -1.16651998e+00]\n",
      " [  5.17158880e-01  -1.19878050e+00  -1.22325043e+00  -1.06662705e+00\n",
      "   -6.93395299e-02   1.16694939e-01  -2.80588805e-01  -8.23388335e-01\n",
      "   -2.33844004e+00   6.34103928e-01]\n",
      " [ -1.40458559e+00   7.13012044e-01  -1.11645831e+00  -1.60895687e+00\n",
      "   -2.34211060e+00   2.63571293e-01   7.50692497e-02  -1.85018015e-01\n",
      "    8.28333285e-01  -4.68760965e-01]\n",
      " [ -4.20734114e-01   2.48053777e-01   2.67865717e+00   2.36886100e-01\n",
      "    6.12128523e-01  -3.10991541e-01  -2.33749421e-02   6.30388580e-01\n",
      "    4.20565482e-02  -1.46997346e+00]\n",
      " [ -6.44166648e-01  -9.34005241e-01   1.71447063e+00   8.42421761e-01\n",
      "    1.07189967e+00   3.13914114e-01  -6.37824643e-01  -1.14826407e+00\n",
      "   -2.12665129e-02  -9.02692783e-01]\n",
      " [ -1.44296367e+00  -4.58349392e-01   6.45182551e-01  -7.84397167e-01\n",
      "   -1.75605582e-01   6.31329345e-01   1.09586564e+00  -1.10517651e+00\n",
      "    1.88564678e+00  -9.18816137e-01]]\n",
      "Bias of layer is:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "XW + b is:\n",
      "[[-0.56370534 -0.58118105 -8.7512024  -7.50467739  2.7565426  -0.46036165\n",
      "  -1.19463141  2.2033753  -4.53234578 -1.32466225]\n",
      " [-0.91879824 -0.63071384 -8.3885346  -6.68024628  2.68192757 -0.98958107\n",
      "  -0.21245297  3.90829123 -2.50633745 -1.12360139]]\n",
      "Logits are:\n",
      "[[-0.56370534 -0.58118105 -8.7512024  -7.50467739  2.7565426  -0.46036165\n",
      "  -1.19463141  2.2033753  -4.53234578 -1.32466225]\n",
      " [-0.91879824 -0.63071384 -8.3885346  -6.68024628  2.68192757 -0.98958107\n",
      "  -0.21245297  3.90829123 -2.50633745 -1.12360139]]\n",
      "After exponents are:\n",
      "[[  5.69096455e-01   5.59237489e-01   1.58270906e-04   5.50503419e-04\n",
      "    1.57453109e+01   6.31055380e-01   3.02815547e-01   9.05552709e+00\n",
      "    1.07554167e-02   2.65892749e-01]\n",
      " [  3.98998251e-01   5.32211749e-01   2.27460355e-04   1.25546873e-03\n",
      "    1.46132342e+01   3.71732387e-01   8.08598348e-01   4.98137589e+01\n",
      "    8.15664341e-02   3.25106847e-01]]\n",
      "Probabilities are:\n",
      "[[  6.04861363e-03   5.94382811e-03   1.68217452e-06   5.85099847e-06\n",
      "    1.67348261e-01   6.70714102e-03   3.21846013e-03   9.62462236e-02\n",
      "    1.14313416e-04   2.82602799e-03]\n",
      " [  4.24073326e-03   5.65658636e-03   2.41755118e-06   1.33436876e-05\n",
      "    1.55316040e-01   3.95093936e-03   8.59414771e-03   5.29443083e-01\n",
      "    8.66924827e-04   3.45538211e-03]]\n",
      "True values y are:\n",
      "[[0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]]\n",
      "Probabilities are:\n",
      "[[  6.04861363e-03   5.94382811e-03   1.68217452e-06   5.85099847e-06\n",
      "    1.67348261e-01   6.70714102e-03   3.21846013e-03   9.62462236e-02\n",
      "    1.14313416e-04   2.82602799e-03]\n",
      " [  4.24073326e-03   5.65658636e-03   2.41755118e-06   1.33436876e-05\n",
      "    1.55316040e-01   3.95093936e-03   8.59414771e-03   5.29443083e-01\n",
      "    8.66924827e-04   3.45538211e-03]]\n",
      "True value y max index are:\n",
      "[5 4]\n",
      "(2,)\n",
      "Probabilities max index are:\n",
      "[4 7]\n",
      "(2,)\n",
      "Log probabilities are:\n",
      "[[ -5.10792618  -5.12540189 -13.29542324 -12.04889823  -1.78767824\n",
      "   -5.0045825   -5.73885226  -2.34084554  -9.07656662  -5.86888309]\n",
      " [ -5.46301909  -5.17493468 -12.93275544 -11.22446712  -1.86229327\n",
      "   -5.53380192  -4.75667381  -0.63592961  -7.05055829  -5.66782223]]\n",
      "Cross entropy is:\n",
      "[[-0.         -0.         -0.         -0.         -0.         -5.0045825\n",
      "  -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -1.86229327 -0.         -0.\n",
      "  -0.         -0.         -0.        ]]\n",
      "Cross entropy sum is\n",
      "6.86687576404\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "X_batch, y_batch = resample(X_train_normalize, y_train_one_hot, n_samples = batch_size)\n",
    "X.value = X_batch\n",
    "y.value = y_batch\n",
    "print(X_batch.shape)\n",
    "print(y_batch.shape)\n",
    "forward_pass(graph, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of cross entropy with respect to y layer is:\n",
      "[[ -1.65327141e+02  -1.68241743e+02  -5.94468639e+05  -1.70911000e+05\n",
      "   -5.97556254e+00  -1.49094823e+02  -3.10707593e+02  -1.03900180e+01\n",
      "   -8.74787959e+03  -3.53853538e+02]\n",
      " [ -2.35808276e+02  -1.76785067e+02  -4.13641709e+05  -7.49418025e+04\n",
      "   -6.43848502e+00  -2.53104365e+02  -1.16358251e+02  -1.88877716e+00\n",
      "   -1.15350255e+03  -2.89403593e+02]]\n",
      "Gradients of cross entropy with respect to softmax layer is:\n",
      "[[   0.            0.            0.            0.            0.\n",
      "  -149.09482251    0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.           -6.43848502\n",
      "     0.            0.            0.            0.            0.        ]]\n",
      "grad_cost is:\n",
      "[[   0.            0.            0.            0.            0.\n",
      "  -149.09482251    0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.           -6.43848502\n",
      "     0.            0.            0.            0.            0.        ]]\n",
      "The Jacobian is:\n",
      "[[  6.01202791e-03  -5.99399937e-05  -2.04270134e-08  -9.19774489e-08\n",
      "   -1.67087887e-03  -5.73237846e-05  -5.59127098e-05  -2.82738311e-03\n",
      "   -4.36783463e-06  -3.17469053e-05]\n",
      " [ -5.99399937e-05   5.90849902e-03  -2.36736432e-08  -1.10257050e-07\n",
      "   -1.87324789e-03  -6.22149230e-05  -6.77435125e-05  -3.56691153e-03\n",
      "   -5.58329445e-06  -3.63430919e-05]\n",
      " [ -2.04270134e-08  -2.36736432e-08   1.68217169e-06  -4.21014482e-11\n",
      "   -6.56993457e-07  -2.08341799e-08  -2.61908035e-08  -1.44185869e-06\n",
      "   -2.28813025e-09  -1.31074354e-08]\n",
      " [ -9.19774489e-08  -1.10257050e-07  -4.21014482e-11   5.85096423e-06\n",
      "   -3.05164314e-06  -9.19635724e-08  -1.33508827e-07  -7.62785960e-06\n",
      "   -1.22368217e-08  -6.26426248e-08]\n",
      " [ -1.67087887e-03  -1.87324789e-03  -6.56993457e-07  -3.05164314e-06\n",
      "    1.39342821e-01  -1.73607264e-03  -1.87341270e-03  -9.83376414e-02\n",
      "   -1.53777483e-04  -1.00960714e-03]\n",
      " [ -5.73237846e-05  -6.22149230e-05  -2.08341799e-08  -9.19635724e-08\n",
      "   -1.73607264e-03   6.66215528e-03  -5.55416224e-05  -2.73733451e-03\n",
      "   -4.19188362e-06  -3.26065734e-05]\n",
      " [ -5.59127098e-05  -6.77435125e-05  -2.61908035e-08  -1.33508827e-07\n",
      "   -1.87341270e-03  -5.55416224e-05   3.20810164e-03  -4.85987669e-03\n",
      "   -7.81839318e-06  -3.87915227e-05]\n",
      " [ -2.82738311e-03  -3.56691153e-03  -1.44185869e-06  -7.62785960e-06\n",
      "   -9.83376414e-02  -2.73733451e-03  -4.85987669e-03   8.69828880e-02\n",
      "   -4.69989587e-04  -2.10142268e-03]\n",
      " [ -4.36783463e-06  -5.58329445e-06  -2.28813025e-09  -1.22368217e-08\n",
      "   -1.53777483e-04  -4.19188362e-06  -7.81839318e-06  -4.69989587e-04\n",
      "    1.14300349e-04  -3.31860945e-06]\n",
      " [ -3.17469053e-05  -3.63430919e-05  -1.31074354e-08  -6.26426248e-08\n",
      "   -1.00960714e-03  -3.26065734e-05  -3.87915227e-05  -2.10142268e-03\n",
      "   -3.31860945e-06   2.81804155e-03]]\n",
      "The derivative of the cost with respect to the inputs of the softmax layer is:\n",
      "[[  8.54667949e-03   9.27592291e-03   3.10626835e-06   1.37112925e-05\n",
      "    2.58839443e-01  -9.93292859e-01   8.28096833e-03   4.08122403e-01\n",
      "    6.24988145e-04   4.86147128e-03]\n",
      " [  1.07579286e-02   1.20608785e-02   4.23004254e-06   1.96479586e-05\n",
      "   -8.97156663e-01   1.11776777e-02   1.20619396e-02   6.33145431e-01\n",
      "    9.90094020e-04   6.50034043e-03]]\n",
      "grad_cost is:\n",
      "[[  8.54667949e-03   9.27592291e-03   3.10626835e-06   1.37112925e-05\n",
      "    2.58839443e-01  -9.93292859e-01   8.28096833e-03   4.08122403e-01\n",
      "    6.24988145e-04   4.86147128e-03]\n",
      " [  1.07579286e-02   1.20608785e-02   4.23004254e-06   1.96479586e-05\n",
      "   -8.97156663e-01   1.11776777e-02   1.20619396e-02   6.33145431e-01\n",
      "    9.90094020e-04   6.50034043e-03]]\n",
      "The derivatives of the cost with respect to the inputs are:\n",
      "[[ 0.08350175 -0.10704727  1.08941581 -0.04167256  0.30955146  0.39502331\n",
      "  -1.36125442 -1.5672437  -0.696456    0.31812523  0.96880064  0.60443646\n",
      "  -0.20381211  0.9440581  -2.02671811  0.3235864  -0.27331307 -3.00938498\n",
      "   1.36620991  0.01844928 -0.3759119  -1.46680802  0.85523235 -0.10602542\n",
      "   2.17585527 -0.47732388 -0.95009983  0.71602748 -0.52682691 -1.13440225]\n",
      " [ 0.41212209 -0.85883765 -0.39414817 -0.33533209 -2.06862801 -0.85092484\n",
      "  -1.96765968 -2.15450034 -0.79510956 -1.13031493  0.85256418  1.63305397\n",
      "   0.07694512 -0.67343495 -0.66161469 -0.33460923  0.31243385  1.48081721\n",
      "   0.19548707 -0.69193763  0.30449651 -1.12822647  0.85777424  0.33197314\n",
      "   0.8137998  -0.46831051  1.97917422 -0.16483783 -1.71692454 -0.54708641]]\n",
      "The derivatives of the cost with respect to the weights are:\n",
      "[[  1.92979037e-02   2.13295171e-02   7.33386759e-06   3.33484461e-05\n",
      "   -6.38494968e-01  -9.81362098e-01   2.03363781e-02   1.04094522e+00\n",
      "    1.61458769e-03   1.13579903e-02]\n",
      " [  2.43439161e-05   2.64225128e-05   8.84894467e-09   3.90635760e-08\n",
      "    7.32634635e-04  -2.82446770e-03   2.35932816e-05   1.16292673e-03\n",
      "    1.78095373e-06   1.38486040e-05]\n",
      " [  1.93046076e-02   2.13368009e-02   7.33631071e-06   3.33592504e-05\n",
      "   -6.38317212e-01  -9.82115151e-01   2.03429075e-02   1.04126781e+00\n",
      "    1.61508213e-03   1.13618114e-02]\n",
      " [  1.92756323e-02   2.13053532e-02   7.32577967e-06   3.33127655e-05\n",
      "   -6.39194633e-01  -9.78747756e-01   2.03148328e-02   1.03988416e+00\n",
      "    1.61296325e-03   1.13453298e-02]\n",
      " [  2.52524685e-03   2.80286021e-03   9.69461308e-07   4.43684600e-06\n",
      "   -1.20918192e-01  -8.98736002e-02   2.71120034e-03   1.39863125e-01\n",
      "    2.17493494e-04   1.49790515e-03]\n",
      " [  1.07129966e-04   1.20103809e-04   4.21227235e-08   1.95651288e-07\n",
      "   -8.93016780e-03   1.07258483e-04   1.20110350e-04   6.30462651e-03\n",
      "    9.85893423e-06   6.47306844e-05]\n",
      " [  1.93045997e-02   2.13367921e-02   7.33630768e-06   3.33592365e-05\n",
      "   -6.38316864e-01  -9.82114839e-01   2.03428990e-02   1.04126738e+00\n",
      "    1.61508145e-03   1.13618068e-02]\n",
      " [  1.03676014e-03   1.12582643e-03   3.77310718e-07   1.66701062e-06\n",
      "    2.94771214e-02  -1.18509952e-01   1.00710229e-03   4.96951890e-02\n",
      "    7.61341616e-05   5.90322485e-04]\n",
      " [  1.08250446e-06   1.21361233e-06   4.25643272e-10   1.97705374e-09\n",
      "   -9.02753802e-05   1.12474125e-06   1.21371911e-06   6.37095469e-05\n",
      "    9.96270972e-08   6.54089445e-07]\n",
      " [  2.25416897e-05   2.44665451e-05   8.19395786e-09   3.61725015e-08\n",
      "    6.77947681e-04  -2.61490423e-03   2.18472270e-05   1.07687686e-03\n",
      "    1.64918095e-06   1.28235053e-05]\n",
      " [  1.85939782e-02   2.05402922e-02   7.05704652e-06   3.20625495e-05\n",
      "   -5.79653901e-01  -9.82235061e-01   1.95469432e-02   9.99503094e-01\n",
      "    1.54977986e-03   1.09326096e-02]\n",
      " [  2.66515122e-03   2.89900090e-03   9.74001451e-07   4.31565713e-06\n",
      "    6.02400261e-02  -2.88623567e-01   2.60972480e-03   1.29266264e-01\n",
      "    1.98298435e-04   1.52235553e-03]\n",
      " [  1.04922455e-02   1.17623116e-02   4.12498879e-06   1.91583452e-05\n",
      "   -8.72759138e-01   8.59018316e-03   1.17610498e-02   6.17290121e-01\n",
      "    9.65269463e-04   6.33910657e-03]\n",
      " [  1.93046081e-02   2.13368014e-02   7.33631087e-06   3.33592511e-05\n",
      "   -6.38317221e-01  -9.82115179e-01   2.03429079e-02   1.04126783e+00\n",
      "    1.61508216e-03   1.13618117e-02]\n",
      " [  4.15003667e-07   4.63325886e-07   1.61566664e-10   7.45905499e-10\n",
      "   -2.84438105e-05  -5.92826888e-06   4.57047811e-07   2.38224666e-05\n",
      "    3.71689660e-08   2.48839491e-07]\n",
      " [  1.93046069e-02   2.13368001e-02   7.33631042e-06   3.33592490e-05\n",
      "   -6.38317122e-01  -9.82115182e-01   2.03429066e-02   1.04126776e+00\n",
      "    1.61508206e-03   1.13618110e-02]\n",
      " [  1.92805830e-02   2.13099305e-02   7.32691733e-06   3.33157692e-05\n",
      "   -6.36516687e-01  -9.81930720e-01   2.03162427e-02   1.03987370e+00\n",
      "    1.61290482e-03   1.13473582e-02]\n",
      " [  3.14923582e-09   3.53065071e-09   1.23828202e-12   5.75163400e-12\n",
      "   -2.62611863e-07   3.25310520e-09   3.53094244e-09   1.85342828e-07\n",
      "    2.89833359e-10   1.90287963e-09]\n",
      " [  8.83860432e-04   9.59278100e-04   3.21238832e-07   1.41797767e-06\n",
      "    2.67601574e-02  -1.02713888e-01   8.56392313e-04   4.22070151e-02\n",
      "    6.46348692e-05   5.02754681e-04]\n",
      " [  1.52883369e-02   1.69778417e-02   5.87660662e-06   2.69160083e-05\n",
      "   -7.59947742e-01  -5.15349094e-01   1.64514952e-02   8.49481843e-01\n",
      "    1.32138597e-03   9.07729911e-03]\n",
      " [  1.90269322e-02   2.10259847e-02   7.22753542e-06   3.28551525e-05\n",
      "   -6.16716677e-01  -9.80798491e-01   2.00336588e-02   1.02507752e+00\n",
      "    1.58978541e-03   1.11945146e-02]\n",
      " [  1.01642267e-02   1.10893799e-02   3.74229168e-06   1.66655324e-05\n",
      "    1.23944181e-01  -9.91612168e-01   1.00945849e-02   5.03321275e-01\n",
      "    7.73857314e-04   5.83885346e-03]\n",
      " [  1.93045508e-02   2.13367372e-02   7.33628841e-06   3.33591470e-05\n",
      "   -6.38312688e-01  -9.82114987e-01   2.03428440e-02   1.04126449e+00\n",
      "    1.61507693e-03   1.13617772e-02]\n",
      " [  3.47554799e-13   3.85889578e-13   1.33533904e-16   6.11435709e-16\n",
      "   -1.70433566e-11  -1.19557141e-11   3.73685384e-13   1.92888174e-11\n",
      "    3.00007853e-14   2.06284694e-13]\n",
      " [  1.93045868e-02   2.13367784e-02   7.33630317e-06   3.33592171e-05\n",
      "   -6.38317864e-01  -9.82112713e-01   2.03428874e-02   1.04126682e+00\n",
      "    1.61508061e-03   1.13617996e-02]\n",
      " [  1.93045788e-02   2.13367697e-02   7.33630023e-06   3.33592041e-05\n",
      "   -6.38318007e-01  -9.82111886e-01   2.03428795e-02   1.04126643e+00\n",
      "    1.61508001e-03   1.13617951e-02]\n",
      " [  1.92929226e-02   2.13241189e-02   7.33206382e-06   3.33405043e-05\n",
      "   -6.38671099e-01  -9.80757118e-01   2.03315857e-02   1.04070982e+00\n",
      "    1.61422764e-03   1.13551648e-02]\n",
      " [  1.79893338e-02   1.98921423e-02   6.84401222e-06   3.11427019e-05\n",
      "   -6.23648095e-01  -8.85472701e-01   1.89955047e-02   9.73138073e-01\n",
      "    1.50983589e-03   1.05966820e-02]\n",
      " [  4.39610904e-03   4.77214638e-03   1.59853614e-06   7.05843809e-06\n",
      "    1.30150988e-01  -5.07833776e-01   4.26343828e-03   2.10215387e-01\n",
      "    3.21968530e-04   2.50149925e-03]\n",
      " [  1.26199889e-05   1.41396505e-05   4.95488268e-09   2.29940873e-08\n",
      "   -1.02445249e-03  -1.57592622e-05   1.41122072e-05   7.40001362e-04\n",
      "    1.15681125e-06   7.61674458e-06]]\n",
      "The derivatives of the cost with respect to the biases are:\n",
      "[  1.93046081e-02   2.13368014e-02   7.33631088e-06   3.33592511e-05\n",
      "  -6.38317221e-01  -9.82115181e-01   2.03429079e-02   1.04126783e+00\n",
      "   1.61508217e-03   1.13618117e-02]\n",
      "The derivatives of the cost with respect to the sigmoid activation is:\n",
      "[[  6.32798185e-05  -3.03532143e-04   3.35144530e-08  -1.40797887e-04\n",
      "    2.59336370e-02   1.59693774e-06  -4.77124970e-07  -1.64699641e-01\n",
      "   -6.70237623e-16   8.35293188e-04   5.97512683e-04   1.24646276e-01\n",
      "   -4.68994822e-04   2.00853618e-09  -1.28608145e-05   6.71060300e-11\n",
      "   -5.71021267e-05  -5.70735788e-11   1.26667348e-01   4.59562475e-03\n",
      "   -6.01141060e-04  -4.53520180e-08   2.16976707e-07  -1.30306544e-12\n",
      "    5.40595187e-06  -1.58410867e-06  -1.29723209e-03   6.31768184e-02\n",
      "   -1.31639570e-01  -3.26801109e-05]\n",
      " [  8.52426871e-06  -3.25154050e-06  -7.28599436e-09  -3.57409484e-08\n",
      "   -2.80000067e-01  -8.38664667e-03  -9.81728738e-07  -3.37988876e-03\n",
      "   -7.99989541e-05  -4.37716713e-06   5.22342053e-02   2.68831369e-02\n",
      "    1.98711278e-03  -3.81084422e-10  -2.21866322e-05  -3.68582295e-08\n",
      "    6.44535413e-04   4.33466459e-07   1.26156150e-06  -2.13871588e-06\n",
      "    7.28862037e-03  -1.44131967e-01   4.39680984e-06   7.48364177e-12\n",
      "    9.27147463e-11  -3.77747349e-08   3.43875881e-08  -7.01878171e-03\n",
      "   -4.18300656e-03  -6.28534901e-04]]\n",
      "grad_cost is:\n",
      "[[  6.32798185e-05  -3.03532143e-04   3.35144530e-08  -1.40797887e-04\n",
      "    2.59336370e-02   1.59693774e-06  -4.77124970e-07  -1.64699641e-01\n",
      "   -6.70237623e-16   8.35293188e-04   5.97512683e-04   1.24646276e-01\n",
      "   -4.68994822e-04   2.00853618e-09  -1.28608145e-05   6.71060300e-11\n",
      "   -5.71021267e-05  -5.70735788e-11   1.26667348e-01   4.59562475e-03\n",
      "   -6.01141060e-04  -4.53520180e-08   2.16976707e-07  -1.30306544e-12\n",
      "    5.40595187e-06  -1.58410867e-06  -1.29723209e-03   6.31768184e-02\n",
      "   -1.31639570e-01  -3.26801109e-05]\n",
      " [  8.52426871e-06  -3.25154050e-06  -7.28599436e-09  -3.57409484e-08\n",
      "   -2.80000067e-01  -8.38664667e-03  -9.81728738e-07  -3.37988876e-03\n",
      "   -7.99989541e-05  -4.37716713e-06   5.22342053e-02   2.68831369e-02\n",
      "    1.98711278e-03  -3.81084422e-10  -2.21866322e-05  -3.68582295e-08\n",
      "    6.44535413e-04   4.33466459e-07   1.26156150e-06  -2.13871588e-06\n",
      "    7.28862037e-03  -1.44131967e-01   4.39680984e-06   7.48364177e-12\n",
      "    9.27147463e-11  -3.77747349e-08   3.43875881e-08  -7.01878171e-03\n",
      "   -4.18300656e-03  -6.28534901e-04]]\n",
      "The derivatives of the cost with respect to the inputs are:\n",
      "[[-0.10543592  0.01857866 -0.21243367 ..., -0.0681494  -0.01293314\n",
      "  -0.57834578]\n",
      " [ 0.0596782  -0.43989212  0.03391069 ..., -0.26120397 -0.13232092\n",
      "  -0.22526223]]\n",
      "The derivatives of the cost with respect to the weights are:\n",
      "[[ -3.59020436e-05   1.53391842e-04  -1.31142293e-08 ...,  -2.80790183e-02\n",
      "    6.79112885e-02   3.30607506e-04]\n",
      " [ -3.59020436e-05   1.53391842e-04  -1.31142293e-08 ...,  -2.80790183e-02\n",
      "    6.79112885e-02   3.30607506e-04]\n",
      " [ -3.59020436e-05   1.53391842e-04  -1.31142293e-08 ...,  -2.80790183e-02\n",
      "    6.79112885e-02   3.30607506e-04]\n",
      " ..., \n",
      " [ -3.59020436e-05   1.53391842e-04  -1.31142293e-08 ...,  -2.80790183e-02\n",
      "    6.79112885e-02   3.30607506e-04]\n",
      " [ -3.59020436e-05   1.53391842e-04  -1.31142293e-08 ...,  -2.80790183e-02\n",
      "    6.79112885e-02   3.30607506e-04]\n",
      " [ -3.59020436e-05   1.53391842e-04  -1.31142293e-08 ...,  -2.80790183e-02\n",
      "    6.79112885e-02   3.30607506e-04]]\n",
      "The derivatives of the cost with respect to the biases are:\n",
      "[  7.18040872e-05  -3.06783684e-04   2.62284587e-08  -1.40833628e-04\n",
      "  -2.54066430e-01  -8.38504973e-03  -1.45885371e-06  -1.68079530e-01\n",
      "  -7.99989541e-05   8.30916021e-04   5.28317179e-02   1.51529413e-01\n",
      "   1.51811796e-03   1.62745176e-09  -3.50474466e-05  -3.67911235e-08\n",
      "   5.87433286e-04   4.33409386e-07   1.26668610e-01   4.59348603e-03\n",
      "   6.68747931e-03  -1.44132012e-01   4.61378655e-06   6.18057633e-12\n",
      "   5.40604459e-06  -1.62188340e-06  -1.29719770e-03   5.61580367e-02\n",
      "  -1.35822577e-01  -6.61215011e-04]\n"
     ]
    }
   ],
   "source": [
    "backward_pass(graph, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of validation set in epoch 1 is: 167581.1664809372\n",
      "0.0995833333333\n",
      "Loss of validation set in epoch 11 is: 152896.68453651306\n",
      "0.117166666667\n",
      "Loss of validation set in epoch 21 is: 148213.0617231116\n",
      "0.125333333333\n",
      "Loss of validation set in epoch 31 is: 146839.50922931568\n",
      "0.132083333333\n",
      "Loss of validation set in epoch 41 is: 145987.8331951922\n",
      "0.146583333333\n",
      "Loss of validation set in epoch 51 is: 144814.23021850205\n",
      "0.15375\n",
      "Loss of validation set in epoch 61 is: 144215.67814227677\n",
      "0.155416666667\n",
      "Loss of validation set in epoch 71 is: 143517.22510926367\n",
      "0.160666666667\n",
      "Loss of validation set in epoch 81 is: 143115.41765396218\n",
      "0.170083333333\n",
      "Loss of validation set in epoch 91 is: 142614.17814602784\n",
      "0.180666666667\n",
      "Loss of validation set in epoch 101 is: 142166.7074845169\n",
      "0.186333333333\n",
      "Loss of validation set in epoch 111 is: 141782.50949392942\n",
      "0.188166666667\n",
      "Loss of validation set in epoch 121 is: 141409.43022277288\n",
      "0.198583333333\n",
      "Loss of validation set in epoch 131 is: 141171.74706112887\n",
      "0.19975\n",
      "Loss of validation set in epoch 141 is: 140829.15248234\n",
      "0.213166666667\n",
      "Loss of validation set in epoch 151 is: 140531.95622163845\n",
      "0.2155\n",
      "Loss of validation set in epoch 161 is: 140333.43187153\n",
      "0.218\n",
      "Loss of validation set in epoch 171 is: 140076.15990847195\n",
      "0.225416666667\n",
      "Loss of validation set in epoch 181 is: 139780.7190914529\n",
      "0.233916666667\n",
      "Loss of validation set in epoch 191 is: 139547.4921629018\n",
      "0.23875\n",
      "Loss of validation set in epoch 201 is: 139307.89820164826\n",
      "0.243\n",
      "Loss of validation set in epoch 211 is: 139092.47428143196\n",
      "0.248\n",
      "Loss of validation set in epoch 221 is: 138847.52940014476\n",
      "0.258583333333\n",
      "Loss of validation set in epoch 231 is: 138700.880925182\n",
      "0.26175\n",
      "Loss of validation set in epoch 241 is: 138596.51229819685\n",
      "0.264\n",
      "Loss of validation set in epoch 251 is: 138398.8374582622\n",
      "0.271333333333\n",
      "Loss of validation set in epoch 261 is: 138118.05912154945\n",
      "0.278\n",
      "Loss of validation set in epoch 271 is: 137894.32140331564\n",
      "0.284416666667\n",
      "Loss of validation set in epoch 281 is: 137765.06454022962\n",
      "0.292833333333\n",
      "Loss of validation set in epoch 291 is: 137510.46488118544\n",
      "0.295416666667\n",
      "Loss of validation set in epoch 301 is: 137491.31376875233\n",
      "0.301666666667\n",
      "Loss of validation set in epoch 311 is: 137262.13153064382\n",
      "0.307166666667\n",
      "Loss of validation set in epoch 321 is: 137094.7886462606\n",
      "0.311\n",
      "Loss of validation set in epoch 331 is: 136938.47611953103\n",
      "0.31425\n",
      "Loss of validation set in epoch 341 is: 136842.94898738575\n",
      "0.320083333333\n",
      "Loss of validation set in epoch 351 is: 136981.87959327718\n",
      "0.33075\n",
      "Loss of validation set in epoch 361 is: 136573.72203760166\n",
      "0.330083333333\n",
      "Loss of validation set in epoch 371 is: 136426.89223809217\n",
      "0.334\n",
      "Loss of validation set in epoch 381 is: 136229.2349307657\n",
      "0.342333333333\n",
      "Loss of validation set in epoch 391 is: 136092.90072400056\n",
      "0.338\n",
      "Loss of validation set in epoch 401 is: 135994.1092501177\n",
      "0.348833333333\n",
      "Loss of validation set in epoch 411 is: 135811.36752411257\n",
      "0.354083333333\n",
      "Loss of validation set in epoch 421 is: 135737.2768632218\n",
      "0.357083333333\n",
      "Loss of validation set in epoch 431 is: 135794.8604465044\n",
      "0.35825\n",
      "Loss of validation set in epoch 441 is: 135693.40037556578\n",
      "0.358583333333\n",
      "Loss of validation set in epoch 451 is: 135509.13294612968\n",
      "0.359083333333\n",
      "Loss of validation set in epoch 461 is: 135393.54346352175\n",
      "0.362166666667\n",
      "Loss of validation set in epoch 471 is: 135317.18992842024\n",
      "0.364666666667\n",
      "Loss of validation set in epoch 481 is: 135196.48355154868\n",
      "0.370916666667\n",
      "Loss of validation set in epoch 491 is: 135094.35188074497\n",
      "0.37375\n",
      "Loss of validation set in epoch 501 is: 135078.45550590532\n",
      "0.370416666667\n",
      "Loss of validation set in epoch 511 is: 134924.6837872666\n",
      "0.375083333333\n",
      "Loss of validation set in epoch 521 is: 134857.45161265586\n",
      "0.383666666667\n",
      "Loss of validation set in epoch 531 is: 134724.83939957063\n",
      "0.386416666667\n",
      "Loss of validation set in epoch 541 is: 134660.39338288884\n",
      "0.38525\n",
      "Loss of validation set in epoch 551 is: 134514.80551829768\n",
      "0.388166666667\n",
      "Loss of validation set in epoch 561 is: 134409.83031494025\n",
      "0.393916666667\n",
      "Loss of validation set in epoch 571 is: 135877.18202300594\n",
      "0.396333333333\n",
      "Loss of validation set in epoch 581 is: 134411.4047582665\n",
      "0.403666666667\n",
      "Loss of validation set in epoch 591 is: 134408.44233511906\n",
      "0.408416666667\n",
      "Loss of validation set in epoch 601 is: 134058.79192023855\n",
      "0.410333333333\n",
      "Loss of validation set in epoch 611 is: 134096.20637721365\n",
      "0.403166666667\n",
      "Loss of validation set in epoch 621 is: 134021.5061905055\n",
      "0.402333333333\n",
      "Loss of validation set in epoch 631 is: 133962.98264465466\n",
      "0.40525\n",
      "Loss of validation set in epoch 641 is: 133829.75301220533\n",
      "0.413\n",
      "Loss of validation set in epoch 651 is: 133777.8670490557\n",
      "0.414083333333\n",
      "Loss of validation set in epoch 661 is: 133675.47053733643\n",
      "0.414083333333\n",
      "Loss of validation set in epoch 671 is: 133656.472989899\n",
      "0.413\n",
      "Loss of validation set in epoch 681 is: 133576.81817948818\n",
      "0.41525\n",
      "Loss of validation set in epoch 691 is: 133595.07884071034\n",
      "0.410833333333\n",
      "Loss of validation set in epoch 701 is: 133679.21421498945\n",
      "0.406416666667\n",
      "Loss of validation set in epoch 711 is: 133492.84637209555\n",
      "0.414583333333\n",
      "Loss of validation set in epoch 721 is: 133532.4273606265\n",
      "0.413833333333\n",
      "Loss of validation set in epoch 731 is: 133503.49270099125\n",
      "0.422166666667\n",
      "Loss of validation set in epoch 741 is: 133317.15988800957\n",
      "0.4275\n",
      "Loss of validation set in epoch 751 is: 133344.77566501172\n",
      "0.43\n",
      "Loss of validation set in epoch 761 is: 133912.08759804675\n",
      "0.428333333333\n",
      "Loss of validation set in epoch 771 is: 133334.91794623068\n",
      "0.425166666667\n",
      "Loss of validation set in epoch 781 is: 133391.1315255774\n",
      "0.423916666667\n",
      "Loss of validation set in epoch 791 is: 133464.96249250238\n",
      "0.423083333333\n",
      "Loss of validation set in epoch 801 is: 133091.31231544612\n",
      "0.428333333333\n",
      "Loss of validation set in epoch 811 is: 132928.0857711505\n",
      "0.433333333333\n",
      "Loss of validation set in epoch 821 is: 133133.86161452628\n",
      "0.434916666667\n",
      "Loss of validation set in epoch 831 is: 133371.6549837747\n",
      "0.434333333333\n",
      "Loss of validation set in epoch 841 is: 132796.0195678744\n",
      "0.43975\n",
      "Loss of validation set in epoch 851 is: 132775.7502611264\n",
      "0.438333333333\n",
      "Loss of validation set in epoch 861 is: 132678.58942718906\n",
      "0.43975\n",
      "Loss of validation set in epoch 871 is: 132616.36506262317\n",
      "0.44075\n",
      "Loss of validation set in epoch 881 is: 132552.11747874998\n",
      "0.442416666667\n",
      "Loss of validation set in epoch 891 is: 132598.52561088672\n",
      "0.442166666667\n",
      "Loss of validation set in epoch 901 is: 132415.76322740794\n",
      "0.446833333333\n",
      "Loss of validation set in epoch 911 is: 132321.3092481713\n",
      "0.449833333333\n",
      "Loss of validation set in epoch 921 is: 132307.58739086564\n",
      "0.449833333333\n",
      "Loss of validation set in epoch 931 is: 132296.77989021677\n",
      "0.453333333333\n",
      "Loss of validation set in epoch 941 is: 132251.43085313006\n",
      "0.453166666667\n",
      "Loss of validation set in epoch 951 is: 132219.79378717477\n",
      "0.450416666667\n",
      "Loss of validation set in epoch 961 is: 132290.62638476386\n",
      "0.447666666667\n",
      "Loss of validation set in epoch 971 is: 132237.2429616943\n",
      "0.447583333333\n",
      "Loss of validation set in epoch 981 is: 132291.40604499856\n",
      "0.4535\n",
      "Loss of validation set in epoch 991 is: 132174.58235905867\n",
      "0.453166666667\n",
      "Loss of validation set in epoch 1001 is: 132274.3186119807\n",
      "0.450333333333\n",
      "Loss of validation set in epoch 1011 is: 132117.2478514321\n",
      "0.453083333333\n",
      "Loss of validation set in epoch 1021 is: 132009.77173998946\n",
      "0.45575\n",
      "Loss of validation set in epoch 1031 is: 131994.71246758112\n",
      "0.459333333333\n",
      "Loss of validation set in epoch 1041 is: 131826.71516247123\n",
      "0.465416666667\n",
      "Loss of validation set in epoch 1051 is: 131753.07776342044\n",
      "0.47425\n",
      "Loss of validation set in epoch 1061 is: 131820.76501794948\n",
      "0.471666666667\n",
      "Loss of validation set in epoch 1071 is: 131822.23073295614\n",
      "0.47425\n",
      "Loss of validation set in epoch 1081 is: 131828.84942713106\n",
      "0.470333333333\n",
      "Loss of validation set in epoch 1091 is: 131806.41467330375\n",
      "0.477583333333\n",
      "Loss of validation set in epoch 1101 is: 131723.05530808453\n",
      "0.475833333333\n",
      "Loss of validation set in epoch 1111 is: 131887.11652583772\n",
      "0.4725\n",
      "Loss of validation set in epoch 1121 is: 131658.30195493659\n",
      "0.476666666667\n",
      "Loss of validation set in epoch 1131 is: 131746.92800842467\n",
      "0.468\n",
      "Loss of validation set in epoch 1141 is: 131699.60758285807\n",
      "0.468083333333\n",
      "Loss of validation set in epoch 1151 is: 131719.9550072951\n",
      "0.470333333333\n",
      "Loss of validation set in epoch 1161 is: 131743.57104147252\n",
      "0.472916666667\n",
      "Loss of validation set in epoch 1171 is: 131616.2008576867\n",
      "0.47475\n",
      "Loss of validation set in epoch 1181 is: 131520.9015152408\n",
      "0.479416666667\n",
      "Loss of validation set in epoch 1191 is: 131484.61277208923\n",
      "0.481916666667\n",
      "Loss of validation set in epoch 1201 is: 131464.33221705572\n",
      "0.476666666667\n",
      "Loss of validation set in epoch 1211 is: 131579.70691524542\n",
      "0.48025\n",
      "Loss of validation set in epoch 1221 is: 132494.2451587886\n",
      "0.478833333333\n",
      "Loss of validation set in epoch 1231 is: 131989.54437571517\n",
      "0.483833333333\n",
      "Loss of validation set in epoch 1241 is: 131463.94763513308\n",
      "0.484166666667\n",
      "Loss of validation set in epoch 1251 is: 131346.14941749402\n",
      "0.489\n",
      "Loss of validation set in epoch 1261 is: 131545.91529959298\n",
      "0.486416666667\n",
      "Loss of validation set in epoch 1271 is: 131274.71750213587\n",
      "0.489333333333\n",
      "Loss of validation set in epoch 1281 is: 131165.565775833\n",
      "0.4875\n",
      "Loss of validation set in epoch 1291 is: 131274.05808614465\n",
      "0.48775\n",
      "Loss of validation set in epoch 1301 is: 131213.5931197401\n",
      "0.489833333333\n",
      "Loss of validation set in epoch 1311 is: 131148.35254733884\n",
      "0.492166666667\n",
      "Loss of validation set in epoch 1321 is: 131178.27444666612\n",
      "0.488083333333\n",
      "Loss of validation set in epoch 1331 is: 131408.2540855304\n",
      "0.488166666667\n",
      "Loss of validation set in epoch 1341 is: 131337.40136443317\n",
      "0.491416666667\n",
      "Loss of validation set in epoch 1351 is: 131295.89247481938\n",
      "0.488333333333\n",
      "Loss of validation set in epoch 1361 is: 131405.86411058175\n",
      "0.483083333333\n",
      "Loss of validation set in epoch 1371 is: 131259.6795611615\n",
      "0.48875\n",
      "Loss of validation set in epoch 1381 is: 131190.20852378555\n",
      "0.493416666667\n",
      "Loss of validation set in epoch 1391 is: 131082.0600259309\n",
      "0.495416666667\n",
      "Loss of validation set in epoch 1401 is: 131106.40067789354\n",
      "0.4955\n",
      "Loss of validation set in epoch 1411 is: 131153.9967313609\n",
      "0.505333333333\n",
      "Loss of validation set in epoch 1421 is: 131226.75686022535\n",
      "0.501083333333\n",
      "Loss of validation set in epoch 1431 is: 131334.64447066758\n",
      "0.494\n",
      "Loss of validation set in epoch 1441 is: 131151.1734320861\n",
      "0.499166666667\n",
      "Loss of validation set in epoch 1451 is: 131413.3383308523\n",
      "0.4995\n",
      "Loss of validation set in epoch 1461 is: 131365.2026496701\n",
      "0.499833333333\n",
      "Loss of validation set in epoch 1471 is: 131111.47433951564\n",
      "0.502666666667\n",
      "Loss of validation set in epoch 1481 is: 131146.45140450564\n",
      "0.4985\n",
      "Loss of validation set in epoch 1491 is: 130930.54532550041\n",
      "0.501416666667\n",
      "Loss of validation set in epoch 1501 is: 131235.49709885998\n",
      "0.494416666667\n",
      "Loss of validation set in epoch 1511 is: 131192.48187477063\n",
      "0.495833333333\n",
      "Loss of validation set in epoch 1521 is: 130898.96565298777\n",
      "0.512\n",
      "Loss of validation set in epoch 1531 is: 130813.77873764481\n",
      "0.51225\n",
      "Loss of validation set in epoch 1541 is: 130894.23641228529\n",
      "0.512166666667\n",
      "Loss of validation set in epoch 1551 is: 130951.80930775622\n",
      "0.51725\n",
      "Loss of validation set in epoch 1561 is: 131019.28267582614\n",
      "0.512\n",
      "Loss of validation set in epoch 1571 is: 130866.84022011061\n",
      "0.512083333333\n",
      "Loss of validation set in epoch 1581 is: 131054.69616189771\n",
      "0.511333333333\n",
      "Loss of validation set in epoch 1591 is: 131286.14323603452\n",
      "0.50825\n",
      "Loss of validation set in epoch 1601 is: 131404.4203002322\n",
      "0.50675\n",
      "Loss of validation set in epoch 1611 is: 131001.84036884905\n",
      "0.511916666667\n",
      "Loss of validation set in epoch 1621 is: 131891.35834862824\n",
      "0.50775\n",
      "Loss of validation set in epoch 1631 is: 131188.09753455993\n",
      "0.50975\n",
      "Loss of validation set in epoch 1641 is: 130879.2249965903\n",
      "0.511833333333\n",
      "Loss of validation set in epoch 1651 is: 130945.83532953748\n",
      "0.51525\n",
      "Loss of validation set in epoch 1661 is: 131053.93245178508\n",
      "0.51325\n",
      "Loss of validation set in epoch 1671 is: 131390.1505209904\n",
      "0.506083333333\n",
      "Loss of validation set in epoch 1681 is: 131200.39082120638\n",
      "0.508083333333\n",
      "Loss of validation set in epoch 1691 is: 131422.80190813655\n",
      "0.506666666667\n",
      "Loss of validation set in epoch 1701 is: 131345.71375756207\n",
      "0.5075\n",
      "Loss of validation set in epoch 1711 is: 131164.58338755323\n",
      "0.515833333333\n",
      "Loss of validation set in epoch 1721 is: 131004.40400888064\n",
      "0.517\n",
      "Loss of validation set in epoch 1731 is: 131177.647496046\n",
      "0.514083333333\n",
      "Loss of validation set in epoch 1741 is: 131504.99876237425\n",
      "0.510916666667\n",
      "Loss of validation set in epoch 1751 is: 131119.66955585298\n",
      "0.516833333333\n",
      "Loss of validation set in epoch 1761 is: 130941.38736815868\n",
      "0.52725\n",
      "Loss of validation set in epoch 1771 is: 133581.831784511\n",
      "0.50675\n",
      "Loss of validation set in epoch 1781 is: 132611.95260045846\n",
      "0.522416666667\n",
      "Loss of validation set in epoch 1791 is: 131824.4181942985\n",
      "0.524833333333\n",
      "Loss of validation set in epoch 1801 is: 131807.41846812156\n",
      "0.5255\n",
      "Loss of validation set in epoch 1811 is: 131563.88325514435\n",
      "0.5395\n",
      "Loss of validation set in epoch 1821 is: 131401.44770975088\n",
      "0.536083333333\n",
      "Loss of validation set in epoch 1831 is: 131134.93549258987\n",
      "0.5375\n",
      "Loss of validation set in epoch 1841 is: 131644.97169586687\n",
      "0.530583333333\n",
      "Loss of validation set in epoch 1851 is: 131189.60215967218\n",
      "0.530666666667\n",
      "Loss of validation set in epoch 1861 is: 132171.7829611392\n",
      "0.5245\n",
      "Loss of validation set in epoch 1871 is: 132021.08260561255\n",
      "0.524\n",
      "Loss of validation set in epoch 1881 is: 132158.26408650455\n",
      "0.531333333333\n",
      "Loss of validation set in epoch 1891 is: 131311.52824271712\n",
      "0.537083333333\n",
      "Loss of validation set in epoch 1901 is: 131693.0998753272\n",
      "0.539166666667\n",
      "Loss of validation set in epoch 1911 is: 131228.09752348062\n",
      "0.543083333333\n",
      "Loss of validation set in epoch 1921 is: 131975.57428598005\n",
      "0.536416666667\n",
      "Loss of validation set in epoch 1931 is: 132888.50942309454\n",
      "0.5175\n",
      "Loss of validation set in epoch 1941 is: 132771.0099213454\n",
      "0.518416666667\n",
      "Loss of validation set in epoch 1951 is: 132203.1864635009\n",
      "0.518666666667\n",
      "Loss of validation set in epoch 1961 is: 131493.60566124748\n",
      "0.522083333333\n",
      "Loss of validation set in epoch 1971 is: 131231.7173642861\n",
      "0.5295\n",
      "Loss of validation set in epoch 1981 is: 131606.71460393647\n",
      "0.526833333333\n",
      "Loss of validation set in epoch 1991 is: 131058.61642703223\n",
      "0.531666666667\n",
      "Loss of validation set in epoch 2001 is: 131623.3835536064\n",
      "0.52925\n",
      "Loss of validation set in epoch 2011 is: 132710.43241981594\n",
      "0.512333333333\n",
      "Loss of validation set in epoch 2021 is: 132403.65893624528\n",
      "0.511833333333\n",
      "Loss of validation set in epoch 2031 is: 133549.72058491598\n",
      "0.504083333333\n",
      "Loss of validation set in epoch 2041 is: 132509.7737331889\n",
      "0.519\n",
      "Loss of validation set in epoch 2051 is: 134658.65917001394\n",
      "0.497583333333\n",
      "Loss of validation set in epoch 2061 is: 132669.35449882902\n",
      "0.512083333333\n",
      "Loss of validation set in epoch 2071 is: 132013.52567902033\n",
      "0.510416666667\n",
      "Loss of validation set in epoch 2081 is: 131812.25296999505\n",
      "0.517166666667\n",
      "Loss of validation set in epoch 2091 is: 133571.92473740308\n",
      "0.503916666667\n",
      "Loss of validation set in epoch 2101 is: 132192.05539235126\n",
      "0.517833333333\n",
      "Loss of validation set in epoch 2111 is: 131767.62828387236\n",
      "0.528916666667\n",
      "Loss of validation set in epoch 2121 is: 131814.4234653209\n",
      "0.518333333333\n",
      "Loss of validation set in epoch 2131 is: 132627.09405353054\n",
      "0.519416666667\n",
      "Loss of validation set in epoch 2141 is: 132052.75506840716\n",
      "0.5195\n",
      "Loss of validation set in epoch 2151 is: 131570.13705138298\n",
      "0.525083333333\n",
      "Loss of validation set in epoch 2161 is: 132888.20211420098\n",
      "0.5245\n",
      "Loss of validation set in epoch 2171 is: 133314.2105460824\n",
      "0.52775\n",
      "Loss of validation set in epoch 2181 is: 132716.73082324694\n",
      "0.526833333333\n",
      "Loss of validation set in epoch 2191 is: 133763.67425502243\n",
      "0.514916666667\n",
      "Loss of validation set in epoch 2201 is: 135961.672049651\n",
      "0.519666666667\n",
      "Loss of validation set in epoch 2211 is: 140731.80551753196\n",
      "0.508333333333\n",
      "Loss of validation set in epoch 2221 is: 134802.81360688066\n",
      "0.504\n",
      "Loss of validation set in epoch 2231 is: 136262.24905700242\n",
      "0.49775\n",
      "Loss of validation set in epoch 2241 is: 134165.7961072392\n",
      "0.506333333333\n",
      "Loss of validation set in epoch 2251 is: 132949.6488086866\n",
      "0.505166666667\n",
      "Loss of validation set in epoch 2261 is: 133303.75941966227\n",
      "0.52075\n",
      "Loss of validation set in epoch 2271 is: 134780.0453500356\n",
      "0.502333333333\n",
      "Loss of validation set in epoch 2281 is: 137170.9321412674\n",
      "0.50475\n",
      "Loss of validation set in epoch 2291 is: 133844.11462247185\n",
      "0.517916666667\n",
      "Loss of validation set in epoch 2301 is: 133981.80682333885\n",
      "0.529166666667\n",
      "Loss of validation set in epoch 2311 is: 137015.72964362477\n",
      "0.485916666667\n",
      "Loss of validation set in epoch 2321 is: 136182.79416005177\n",
      "0.489833333333\n",
      "Loss of validation set in epoch 2331 is: 135888.29094176803\n",
      "0.4905\n",
      "Loss of validation set in epoch 2341 is: 138943.36784376964\n",
      "0.500166666667\n",
      "Loss of validation set in epoch 2351 is: 137665.19115979274\n",
      "0.50275\n",
      "Loss of validation set in epoch 2361 is: 137818.19238480978\n",
      "0.50875\n",
      "Loss of validation set in epoch 2371 is: 137002.56267414702\n",
      "0.493833333333\n",
      "Loss of validation set in epoch 2381 is: 147404.63626275765\n",
      "0.48\n",
      "Loss of validation set in epoch 2391 is: 141056.469490993\n",
      "0.49425\n",
      "Loss of validation set in epoch 2401 is: 146238.87606609694\n",
      "0.437333333333\n",
      "Loss of validation set in epoch 2411 is: 139294.15310708806\n",
      "0.437916666667\n",
      "Loss of validation set in epoch 2421 is: 136990.27255798833\n",
      "0.451083333333\n",
      "Loss of validation set in epoch 2431 is: 153907.43528536125\n",
      "0.43775\n",
      "Loss of validation set in epoch 2441 is: 143315.70935877424\n",
      "0.456416666667\n",
      "Loss of validation set in epoch 2451 is: 147951.841226268\n",
      "0.462166666667\n",
      "Loss of validation set in epoch 2461 is: 139794.64294621162\n",
      "0.463083333333\n",
      "Loss of validation set in epoch 2471 is: 168256.2174152031\n",
      "0.346416666667\n",
      "Loss of validation set in epoch 2481 is: 154412.65849221035\n",
      "0.400666666667\n",
      "Loss of validation set in epoch 2491 is: 167635.68175876245\n",
      "0.376083333333\n",
      "Loss of validation set in epoch 2501 is: 151078.0826564327\n",
      "0.410083333333\n",
      "Loss of validation set in epoch 2511 is: 158643.22842970025\n",
      "0.385416666667\n",
      "Loss of validation set in epoch 2521 is: 145742.24284065675\n",
      "0.4095\n",
      "Loss of validation set in epoch 2531 is: 177842.6084248428\n",
      "0.344916666667\n",
      "Loss of validation set in epoch 2541 is: 150673.47892122375\n",
      "0.414583333333\n",
      "Loss of validation set in epoch 2551 is: 167948.89059215103\n",
      "0.4305\n",
      "Loss of validation set in epoch 2561 is: 161399.0846779832\n",
      "0.421166666667\n",
      "Loss of validation set in epoch 2571 is: 1173724.8259036886\n",
      "0.09825\n",
      "Loss of validation set in epoch 2581 is: 1575177.5524002304\n",
      "0.09825\n",
      "Loss of validation set in epoch 2591 is: 1512641.5414418543\n",
      "0.09825\n",
      "Loss of validation set in epoch 2601 is: 1455453.946208837\n",
      "0.09825\n",
      "Loss of validation set in epoch 2611 is: 1396656.7713936197\n",
      "0.09825\n",
      "Loss of validation set in epoch 2621 is: 1341059.7280575563\n",
      "0.09825\n",
      "Loss of validation set in epoch 2631 is: 1282266.5280801514\n",
      "0.09825\n",
      "Loss of validation set in epoch 2641 is: 1208636.5671229595\n",
      "0.09825\n",
      "Loss of validation set in epoch 2651 is: 1282958.2117925098\n",
      "0.09825\n",
      "Loss of validation set in epoch 2661 is: 1216029.0914221562\n",
      "0.09825\n",
      "Loss of validation set in epoch 2671 is: 1158861.4337417083\n",
      "0.09825\n",
      "Loss of validation set in epoch 2681 is: 1077177.679413416\n",
      "0.09825\n",
      "Loss of validation set in epoch 2691 is: 228631.1180806133\n",
      "0.113083333333\n",
      "Loss of validation set in epoch 2701 is: 430788.6790040887\n",
      "0.09825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n",
      "//anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "//anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "//anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "//anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "//anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:45: RuntimeWarning: divide by zero encountered in true_divide\n",
      "//anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of validation set in epoch 2711 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2721 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2731 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2741 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2751 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2761 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2771 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2781 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2791 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2801 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2811 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2821 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2831 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2841 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2851 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2861 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2871 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2881 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2891 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2901 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2911 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2921 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2931 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2941 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2951 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2961 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2971 is: nan\n",
      "0.1035\n",
      "Loss of validation set in epoch 2981 is: nan\n",
      "0.1035\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-981-662981bd164a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_validation_normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_validation_one_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mshow_per_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-858-e25256d12fde>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(graph, debug)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-829-8781d2b823e6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, debug)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now lets run the model\n",
    "epochs = 5000\n",
    "steps_per_epch = 50\n",
    "batch_size = 64\n",
    "show_per_step = 10\n",
    "trainables = [W1, b1, W2, b2]\n",
    "accuracies = []\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Sample a random batch of data \n",
    "        X_batch, y_batch = resample(X_train_normalize, y_train_one_hot, n_samples = batch_size)\n",
    "        \n",
    "        # Reset the values of X and y \n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "#         print(X.value)\n",
    "#         print(y.value)\n",
    "        \n",
    "        # Now run the forward and backward propagation\n",
    "#         if (j%200 == 0):\n",
    "#             print(\"j is:\")\n",
    "#             print(j)\n",
    "#             forward_pass(graph, debug = True) \n",
    "#             backward_pass(graph, debug = True)\n",
    "#             print(\"Loss is {0}\".format(graph[-1].value))\n",
    "        forward_pass(graph)\n",
    "        backward_pass(graph)\n",
    "        \n",
    "        # Update the weights of or biases and weights\n",
    "        sgd_update(trainables, learning_rate = 1e-5) \n",
    "        loss += graph[-1].value\n",
    "\n",
    "#     print(\"Epoch: {}, Loss {:.3f}\".format(i + 1, loss/steps_per_epoch))\n",
    "#     print(\"Average loss per sample is: {0}\".format(loss/(steps_per_epoch * batch_size)))\n",
    "    \n",
    "    # Use the validation set to see our accuracy\n",
    "    X.value = X_validation_normalize\n",
    "    y.value = y_validation_one_hot \n",
    "    forward_pass(graph)\n",
    "    accuracies.append(graph[-1].accuracy)\n",
    "    if (i%show_per_step == 0):\n",
    "        print(\"Loss of validation set in epoch {0} is: {1}\".format(i + 1, graph[-1].value)) \n",
    "        print(graph[-1].accuracy)\n",
    " \n",
    "\n",
    "# # Test it on the test set\n",
    "# X.value = X_test\n",
    "# y.value = y_test\n",
    "# forward_pass(graph)\n",
    "# print(\"Loss of test set is: {0}\".format(graph[-1].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  3.],\n",
       "       [ 3.,  3.]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
