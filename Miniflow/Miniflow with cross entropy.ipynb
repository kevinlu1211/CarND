{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Creating a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for layers in the network\n",
    "    \n",
    "    Arguments:\n",
    "        `inbound_layers`: A list of layers with edges into this class\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_layers = []):\n",
    "        \n",
    "        # The list of layers with edges into the class\n",
    "        self.inbound_layers = inbound_layers\n",
    "        \n",
    "        # The value of this layer which is calculated during the forward pass\n",
    "        self.value = None\n",
    "        \n",
    "        # The layers that the this layer outputs to\n",
    "        self.outbound_layers = []\n",
    "        \n",
    "        # The gradients for this layer\n",
    "        # The keys are the input to this layer and their values are the \n",
    "        # partials of this layer with respect to that layer \n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Sets this layer as an outbound layer for all of this layer's inputs\n",
    "        for layer in inbound_layers: \n",
    "            layer.outbound_layers.append(self)\n",
    "        \n",
    "    def forward(debug = False):\n",
    "        # Abstract method that should be implemented for all the derived classes\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward():\n",
    "        # Abstract method that should be implemented for all the derived classes \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    This layer accepts inputs to the neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Note here that nothing is set because these values are set during\n",
    "        # the topological sort\n",
    "        Layer.__init__(self)\n",
    "        \n",
    "    def forward(self, debug = False):\n",
    "        # Do nothing because nothing is calculated\n",
    "        pass\n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "        # An Input layer has no inputs so the gradient is zero \n",
    "        self.gradients = {self : 0}\n",
    "        \n",
    "        # Weights and bias may be inputs, so we need to sum the gradients \n",
    "        # from their outbound layers during the backward pass.\n",
    "        \n",
    "        # Remember that the goal is to figure out the total change in the cost function\n",
    "        # with respect to a single parameter, hence the addition\n",
    "  \n",
    "        for n in self.outbound_layers:\n",
    "#             a = self.gradients[self]\n",
    "#             print(a)\n",
    "#             b = n.gradients[self]\n",
    "#             print(b)\n",
    "            self.gradients[self] += n.gradients[self] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, X, W, b):\n",
    "        Layer.__init__(self, [X, W, b])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        X = self.inbound_layers[0].value\n",
    "        W = self.inbound_layers[1].value\n",
    "        b = self.inbound_layers[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "       \n",
    "        if debug:\n",
    "            print(\"Input to layer is:\")\n",
    "            print(X)\n",
    "\n",
    "            print(\"Weights of layer is:\")\n",
    "            print(W)\n",
    "\n",
    "            print(\"Bias of layer is:\")\n",
    "            print(b)\n",
    "            \n",
    "            print(\"XW + b is:\")\n",
    "            print(self.value)\n",
    "            \n",
    "    def backward(self, debug = False):\n",
    "        \n",
    "        # Initialize a partial derivative for each of the inbound_layers,\n",
    "        # remembering here that this dictionary stores the partial derivative of\n",
    "        # this layer with respect to the inbound layers\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        \n",
    "        for n in self.outbound_layers:\n",
    "            # Get the partial derivative for each of the variables in this layer \n",
    "            # with respect to the cost\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            if debug:\n",
    "                print(\"grad_cost is:\") \n",
    "                print(grad_cost)\n",
    "                \n",
    "            \n",
    "            \n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, self.inbound_layers[1].value.T) \n",
    "           \n",
    "            self.gradients[self.inbound_layers[1]] += np.dot(self.inbound_layers[0].value.T, grad_cost)\n",
    "              \n",
    "            self.gradients[self.inbound_layers[2]] += np.sum(grad_cost, axis = 0, keepdims = False)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"The derivatives of the cost with respect to the inputs are:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "            \n",
    "            print(\"The derivatives of the cost with respect to the weights are:\")\n",
    "            print(self.gradients[self.inbound_layers[1]])\n",
    "            \n",
    "            print(\"The derivatives of the cost with respect to the biases are:\")\n",
    "            print(self.gradients[self.inbound_layers[2]])\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self, layer):\n",
    "        Layer.__init__(self, [layer])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1. + np.exp(-x))\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        \n",
    "        self.value = self._sigmoid(self.inbound_layers[0].value)\n",
    "            \n",
    "        if debug:\n",
    "            print(\"Input to sigmoid layer is:\")\n",
    "            print(self.inbound_layers[0].value)\n",
    "            \n",
    "            print(\"Value after sigmoid activation is:\")\n",
    "            print(self.value)\n",
    "            \n",
    "        \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients = {n : np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        \n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_layers[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "        \n",
    "        if debug:\n",
    "            print(\"The derivatives of the cost with respect to the sigmoid activation is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MSE(Layer):\n",
    "    def __init__(self, y, a):\n",
    "        Layer.__init__(self, [y, a])\n",
    "        \n",
    "    def forward(self, debug = False):\n",
    "        y = self.inbound_layers[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_layers[1].value.reshape(-1, 1) \n",
    "    \n",
    "        # get the number of samples\n",
    "        self.m = self.inbound_layers[0].value.shape[0] \n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    " \n",
    "        if debug:\n",
    "            print(\"True value of y is:\")\n",
    "            print(y)\n",
    "\n",
    "            print(\"Predicted value of y is:\")\n",
    "            print(a)\n",
    "             \n",
    "            print(\"y - a is:\")\n",
    "            print(self.diff)\n",
    "\n",
    "       \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients[self.inbound_layers[0]] = (2/self.m) * self.diff\n",
    "        self.gradients[self.inbound_layers[1]] = (-2/self.m) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self, logits):\n",
    "        Layer.__init__(self, [logits])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        \n",
    "        exp_logits = np.exp(self.inbound_layers[0].value)\n",
    "        sum_exp = np.sum(exp_logits)  \n",
    "        self.value = exp_logits/sum_exp\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Logits are:\")\n",
    "            print(self.inbound_layers[0].value) \n",
    "            \n",
    "            print(\"After exponents are:\")\n",
    "            exp_logits = np.exp(self.inbound_layers[0].value)\n",
    "            print(exp_logits)\n",
    "            \n",
    "            print(\"Probabilities are:\")\n",
    "            sum_exp = np.sum(exp_logits)  \n",
    "            self.value = exp_logits/sum_exp \n",
    "            print(self.value)\n",
    "             \n",
    "            \n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "         \n",
    "        # Define gradient for inbound layers\n",
    "        self.gradients = {n : np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        jacobian = self._calc_jacobian(self.value)\n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            if debug:\n",
    "                print(\"grad_cost is:\")\n",
    "                print(grad_cost)\n",
    "                print(\"The Jacobian is:\")\n",
    "                print(jacobian)\n",
    "            \n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, jacobian)\n",
    "        \n",
    " \n",
    "        if debug: \n",
    "            print(\"The derivative of the cost with respect to the inputs of the softmax layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "       \n",
    "    def _calc_jacobian(self, probs):\n",
    "        \n",
    "        # First calculate the off diagonal derivatives\n",
    "        jacobian = np.dot(-1 * probs.T, probs)\n",
    "        dims = jacobian.shape[0]\n",
    "        \n",
    "        # Now calculate the diagonal derivatives\n",
    "        for i in range(dims):\n",
    "            jacobian[i,i] = probs[0,i] * (1 - probs[0,i])\n",
    "        return(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Layer):\n",
    "    def __init__(self, y, probs):\n",
    "        Layer.__init__(self, [y, probs])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        n_samples_in_batch = self.inbound_layers[0].value.shape[0]\n",
    "        n_classes = self.inbound_layers[0].value.shape[1]\n",
    "        \n",
    "        self.y_flat = self.inbound_layers[0].value.reshape(n_samples_in_batch, n_classes)\n",
    "        self.probs_flat = self.inbound_layers[1].value.reshape(n_samples_in_batch, n_classes)\n",
    "       \n",
    "        # Calculate the accuracy\n",
    "        n_correct = np.sum(np.argmax(self.y_flat, axis = 1) == np.argmax(self.probs_flat, axis = 1))\n",
    "        self.accuracy = n_correct/n_samples_in_batch\n",
    "        \n",
    "        # Calculate the cross entropy\n",
    "        self.log_probs = np.log(self.probs_flat)\n",
    "        self.cross_entropy = self.y_flat * self.log_probs\n",
    "        self.value = -1 * np.sum(self.cross_entropy)\n",
    "       \n",
    "        \n",
    "        if debug:\n",
    "            print(\"True values y are:\")\n",
    "            print(self.y_flat)\n",
    "            print(\"Probabilities are:\")\n",
    "            print(self.probs_flat)\n",
    "            print(\"True value y max index are:\")\n",
    "            print(np.argmax(self.y_flat, axis = 1))\n",
    "            print(np.argmax(self.y_flat, axis = 1).shape)\n",
    "            print(\"Probabilities max index are:\")\n",
    "            print(np.argmax(self.probs_flat, axis = 1))\n",
    "            print(np.argmax(self.probs_flat, axis = 1).shape) \n",
    "            print(\"Log probabilities are:\")\n",
    "            print(self.log_probs)\n",
    "            print(\"Cross entropy is:\")\n",
    "            print(self.cross_entropy)\n",
    "            print(\"Cross entropy sum is\")\n",
    "            print(self.value)\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients[self.inbound_layers[0]] = -1 * 1/self.probs_flat\n",
    "        self.gradients[self.inbound_layers[1]] = -1 * self.y_flat/self.probs_flat\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Gradients of cross entropy with respect to y layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "            print(\"Gradients of cross entropy with respect to softmax layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "    \n",
    "    G = {}\n",
    "    \n",
    "    layers = [n for n in input_layers]\n",
    "    \n",
    "    # Think of each element in the layer as a node, in this while loop\n",
    "    # we are simply finding which layers are connected to which other layers\n",
    "    while len(layers) > 0:\n",
    "        # Get the first element of the array\n",
    "        n = layers.pop(0)\n",
    "        \n",
    "        # Check if this layer n is in the dictionary if it isn't add it in\n",
    "        if n not in G:\n",
    "            G[n] = {'in' : set(), 'out' : set()}\n",
    "        # Check if this layer m is in the dictionary if it isn't add it in \n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in' : set(), 'out' : set()}\n",
    "            # Add the edges between the nodes\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "        \n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    \n",
    "    while len(S) > 0:\n",
    "        # Get the last layer \n",
    "        n = S.pop()\n",
    "        \n",
    "        # Check if it is an input layer, if it is then initialize the value\n",
    "        if (isinstance(n, Input)):\n",
    "            n.value = feed_dict[n]\n",
    "            \n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            \n",
    "            # if there are no incoming edges to m then add it to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_pass(graph, debug = False):\n",
    "    for n in graph:\n",
    "        n.forward(debug)\n",
    "\n",
    "def backward_pass(graph, debug = False):\n",
    "    for n in graph[::-1]:\n",
    "        n.backward(debug) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_update(trainable, learning_rate = 1e-2): \n",
    "     \n",
    "    for t in trainable:\n",
    "        partial = t.gradients[t]\n",
    "#         print(\"Partial derivatives are:\")\n",
    "#         print(partial)\n",
    "        t.value -= learning_rate * partial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a neural network to classify numbers from the NMIST data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import helper functions\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Just using keras to import the data\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Use one hot encoding for the y label vector\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "y_train_one_hot = lb.transform(y_train)\n",
    "y_test_one_hot = lb.transform(y_test)\n",
    "print(y_train_one_hot.shape)\n",
    "print(y_test_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114120fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Print the first image from the training set out\n",
    "X_batch, y_batch = resample(X_train, y_train_one_hot, n_samples = 1)\n",
    "plt.imshow(X_batch[0], cmap = plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the grayscale image so that the values range between -0.5 and 0.5, \n",
    "# this is so that the sigmoid activation function does not saturate during training \n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [-0.5, 0.5]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    img_min = np.min(image_data)\n",
    "    img_max = np.max(image_data)\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    scaled_img = a + ((image_data - img_min) * (b-a))/(img_max - img_min)\n",
    "    return(scaled_img)\n",
    "\n",
    "X_train_normalize = normalize_grayscale(X_train)\n",
    "X_test_normalize = normalize_grayscale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      "  -0.42941176 -0.42941176 -0.42941176 -0.00588235  0.03333333  0.18627451\n",
      "  -0.39803922  0.15098039  0.5         0.46862745 -0.00196078 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.38235294 -0.35882353 -0.13137255  0.10392157  0.16666667\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.49215686  0.38235294\n",
      "   0.1745098   0.49215686  0.44901961  0.26470588 -0.24901961 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.30784314  0.43333333  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.48431373 -0.13529412\n",
      "  -0.17843137 -0.17843137 -0.28039216 -0.34705882 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.42941176  0.35882353  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.27647059  0.21372549  0.46862745  0.44509804 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.18627451  0.11176471 -0.08039216  0.49215686  0.49215686\n",
      "   0.30392157 -0.45686275 -0.5        -0.33137255  0.10392157 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.44509804 -0.49607843  0.10392157  0.49215686\n",
      "  -0.14705882 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5         0.04509804  0.49215686\n",
      "   0.24509804 -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.45686275  0.24509804\n",
      "   0.49215686 -0.2254902  -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.3627451\n",
      "   0.44509804  0.38235294  0.12745098 -0.07647059 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.18235294  0.44117647  0.49215686  0.49215686 -0.03333333 -0.40196078\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.32352941  0.22941176  0.49215686  0.49215686  0.08823529 -0.39411765\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.4372549  -0.13529412  0.48823529  0.49215686  0.23333333\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5         0.47647059  0.49215686  0.47647059\n",
      "  -0.24901961 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.31960784  0.00980392  0.21764706  0.49215686  0.49215686  0.31176471\n",
      "  -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.34705882\n",
      "   0.08039216  0.39803922  0.49215686  0.49215686  0.49215686  0.48039216\n",
      "   0.21372549 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.40588235 -0.05294118  0.36666667\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.28823529 -0.19411765\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.40980392 -0.24117647  0.33529412  0.49215686  0.49215686\n",
      "   0.49215686  0.49215686  0.27647059 -0.18235294 -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.42941176  0.17058824  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.26470588 -0.18627451 -0.46470588 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.28431373  0.1745098\n",
      "   0.38627451  0.49215686  0.49215686  0.49215686  0.49215686  0.45686275\n",
      "   0.02156863 -0.45686275 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5         0.03333333  0.49215686\n",
      "   0.49215686  0.49215686  0.33137255  0.02941176  0.01764706 -0.4372549\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the data and before and after the reshape to see if we did it properly \n",
    "print(X_train[0])\n",
    "print(X_train_normalize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now flatten the array into a 1 dimension array\n",
    "X_train_normalize = np.array(X_train_normalize).reshape(60000, 28 * 28)\n",
    "X_test_normalize = np.array(X_test_normalize).reshape(10000, 28 * 28 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      " -0.42941176 -0.42941176 -0.42941176 -0.00588235  0.03333333  0.18627451\n",
      " -0.39803922  0.15098039  0.5         0.46862745 -0.00196078 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.38235294 -0.35882353 -0.13137255\n",
      "  0.10392157  0.16666667  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.38235294  0.1745098   0.49215686  0.44901961  0.26470588\n",
      " -0.24901961 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.30784314\n",
      "  0.43333333  0.49215686  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.49215686  0.48431373 -0.13529412 -0.17843137\n",
      " -0.17843137 -0.28039216 -0.34705882 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.42941176  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.27647059  0.21372549  0.46862745  0.44509804\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.18627451  0.11176471\n",
      " -0.08039216  0.49215686  0.49215686  0.30392157 -0.45686275 -0.5\n",
      " -0.33137255  0.10392157 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.44509804 -0.49607843  0.10392157  0.49215686 -0.14705882 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5         0.04509804  0.49215686  0.24509804 -0.49215686\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.45686275  0.24509804  0.49215686\n",
      " -0.2254902  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.3627451\n",
      "  0.44509804  0.38235294  0.12745098 -0.07647059 -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.18235294  0.44117647  0.49215686  0.49215686 -0.03333333 -0.40196078\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.32352941  0.22941176  0.49215686  0.49215686\n",
      "  0.08823529 -0.39411765 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.4372549  -0.13529412\n",
      "  0.48823529  0.49215686  0.23333333 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  0.47647059  0.49215686  0.47647059 -0.24901961 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.31960784  0.00980392\n",
      "  0.21764706  0.49215686  0.49215686  0.31176471 -0.49215686 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.34705882  0.08039216  0.39803922\n",
      "  0.49215686  0.49215686  0.49215686  0.48039216  0.21372549 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.40588235 -0.05294118  0.36666667  0.49215686\n",
      "  0.49215686  0.49215686  0.49215686  0.28823529 -0.19411765 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.40980392 -0.24117647  0.33529412  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.27647059 -0.18235294 -0.49215686 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.42941176  0.17058824  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.26470588 -0.18627451 -0.46470588 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.28431373  0.1745098   0.38627451  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.45686275  0.02156863 -0.45686275 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5         0.03333333  0.49215686  0.49215686  0.49215686  0.33137255\n",
      "  0.02941176  0.01764706 -0.4372549  -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5       ]\n"
     ]
    }
   ],
   "source": [
    "# See if we did the reshaping correctly\n",
    "print(X_train_normalize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Now define a validation set\n",
    "X_train_normalize, X_validation_normalize, y_train_one_hot, y_validation_one_hot = train_test_split(X_train_normalize, y_train_one_hot, test_size = 0.2)\n",
    "\n",
    "# Check that the shape is correct\n",
    "print(X_train_normalize.shape)\n",
    "print(y_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets define the network again!\n",
    "n_hidden = 10\n",
    "# n_hidden_2 = 15\n",
    "n_classes = 10\n",
    "n_features = X_train_normalize.shape[1]\n",
    "\n",
    "# Initialize the weights \n",
    "W1_ = np.random.normal(loc = 0, scale = 0.1, size = (n_features, n_hidden))\n",
    "b1_ = np.zeros(n_hidden)\n",
    "# W2_ = np.random.normal(loc = 0, scale = 0.1, size = (n_hidden, n_hidden_2))\n",
    "# b2_ = np.zeros(n_hidden_2)\n",
    "# W3_ = np.random.normal(loc = 0, scale = 0.1, size = (n_hidden_2, n_classes))\n",
    "# b3_ = np.zeros(n_classes)\n",
    "\n",
    "# W2_ = np.random.normal(loc = 0, scale = 1, size = (n_hidden, n_classes))\n",
    "# b2_ = np.zeros(n_classes)\n",
    "# Build the layers for the neural network\n",
    "X, y, = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "# W3, b3 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "# s1 = Sigmoid(l1)\n",
    "# l2 = Linear(s1, W2, b2)\n",
    "# s2 = Sigmoid(l2)\n",
    "# l3 = Linear(s2, W3, b3)\n",
    "probs = Softmax(l1)\n",
    "cost = CrossEntropy(y, probs)\n",
    "\n",
    "# Define the input layers to the neural network \n",
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    y: y_train_one_hot,\n",
    "    W1: W1_,\n",
    "    b1: b1_\n",
    "#     W2: W2_,\n",
    "#     b2: b2_\n",
    "#     W3: W3_,\n",
    "#     b3: b3_\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "(10, 10)\n",
      "Input to layer is:\n",
      "[[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " ..., \n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "Weights of layer is:\n",
      "[[-0.08796206  0.08463987 -0.00994625 ...,  0.37643793  0.0587617\n",
      "   0.03541774]\n",
      " [-0.0041076  -0.19902142  0.00577772 ..., -0.02267947 -0.09679757\n",
      "   0.08151009]\n",
      " [ 0.06195987 -0.04416678 -0.04204271 ...,  0.18192127 -0.00449183\n",
      "  -0.02671559]\n",
      " ..., \n",
      " [-0.13568438  0.17413247  0.02122802 ..., -0.19021672 -0.06155121\n",
      "   0.07494688]\n",
      " [ 0.17141302  0.07012728  0.08747335 ...,  0.09760939  0.04182569\n",
      "   0.03106224]\n",
      " [-0.06340463  0.01264568 -0.18269752 ...,  0.06715128  0.0323103\n",
      "   0.24029005]]\n",
      "Bias of layer is:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "XW + b is:\n",
      "[[-0.49725211 -1.96026738  0.28146584  0.76743389  1.24041377  1.24628434\n",
      "  -0.00515783 -0.60634674  0.98238163 -0.81323223]\n",
      " [-0.43527494 -1.06847943 -0.1672421  -0.01423385  0.75858048  0.40895021\n",
      "  -0.39035773 -1.71705262 -0.20021139  0.25558692]\n",
      " [-2.27012584 -1.74840251 -0.66998571  1.11406801 -1.68634793 -1.94968995\n",
      "   0.48664784 -1.00712992  0.80518862 -1.18820624]\n",
      " [ 0.14689484 -1.42161461 -0.07409553  0.18389076 -0.878569   -1.28297134\n",
      "  -1.6366746  -1.30925675  1.27962925  0.47034329]\n",
      " [ 0.09869134 -0.82044184  0.76494915  0.23129183  0.80238191  0.86044318\n",
      "  -0.65123152 -0.65276038  1.15831882 -0.17074548]\n",
      " [ 0.01847654 -2.79520785 -0.24865543  0.98123112  0.44657045  2.10505552\n",
      "   0.22363081 -0.63045078  0.66596079 -0.4718869 ]\n",
      " [-1.7387175  -1.40555103  0.81022634  2.0487714  -0.568391   -0.51340743\n",
      "  -0.00510735 -1.49956978 -0.56199489 -0.58761858]\n",
      " [-0.70121163 -1.77586341  0.5925401   0.53164652  0.3751076   0.41499933\n",
      "   0.22185532 -1.42009043 -0.16726683 -0.11909755]\n",
      " [ 0.0117013  -0.80074859  0.22765925  0.68732435 -0.01553505  1.45191959\n",
      "   0.08741835 -1.25342459  0.52001862  0.71392872]\n",
      " [-1.04744354 -0.48281238  0.23505949  1.1831879  -1.33195362  0.03059461\n",
      "  -1.21769932 -1.22890897  0.4241689  -0.35048522]]\n",
      "Logits are:\n",
      "[[-0.49725211 -1.96026738  0.28146584  0.76743389  1.24041377  1.24628434\n",
      "  -0.00515783 -0.60634674  0.98238163 -0.81323223]\n",
      " [-0.43527494 -1.06847943 -0.1672421  -0.01423385  0.75858048  0.40895021\n",
      "  -0.39035773 -1.71705262 -0.20021139  0.25558692]\n",
      " [-2.27012584 -1.74840251 -0.66998571  1.11406801 -1.68634793 -1.94968995\n",
      "   0.48664784 -1.00712992  0.80518862 -1.18820624]\n",
      " [ 0.14689484 -1.42161461 -0.07409553  0.18389076 -0.878569   -1.28297134\n",
      "  -1.6366746  -1.30925675  1.27962925  0.47034329]\n",
      " [ 0.09869134 -0.82044184  0.76494915  0.23129183  0.80238191  0.86044318\n",
      "  -0.65123152 -0.65276038  1.15831882 -0.17074548]\n",
      " [ 0.01847654 -2.79520785 -0.24865543  0.98123112  0.44657045  2.10505552\n",
      "   0.22363081 -0.63045078  0.66596079 -0.4718869 ]\n",
      " [-1.7387175  -1.40555103  0.81022634  2.0487714  -0.568391   -0.51340743\n",
      "  -0.00510735 -1.49956978 -0.56199489 -0.58761858]\n",
      " [-0.70121163 -1.77586341  0.5925401   0.53164652  0.3751076   0.41499933\n",
      "   0.22185532 -1.42009043 -0.16726683 -0.11909755]\n",
      " [ 0.0117013  -0.80074859  0.22765925  0.68732435 -0.01553505  1.45191959\n",
      "   0.08741835 -1.25342459  0.52001862  0.71392872]\n",
      " [-1.04744354 -0.48281238  0.23505949  1.1831879  -1.33195362  0.03059461\n",
      "  -1.21769932 -1.22890897  0.4241689  -0.35048522]]\n",
      "After exponents are:\n",
      "[[ 0.60819963  0.14082076  1.32507073  2.15423116  3.4570436   3.47739808\n",
      "   0.99485545  0.5453395   2.67080957  0.4434225 ]\n",
      " [ 0.64708673  0.34353048  0.84599477  0.98586697  2.13524305  1.50523677\n",
      "   0.67681471  0.1795947   0.8185577   1.29121924]\n",
      " [ 0.10329918  0.17405177  0.51171589  3.04672733  0.18519463  0.14231819\n",
      "   1.6268536   0.36526582  2.23711842  0.30476745]\n",
      " [ 1.15823216  0.24132406  0.92858298  1.20188452  0.41537689  0.27721238\n",
      "   0.19462618  0.27002067  3.59530652  1.60054355]\n",
      " [ 1.10372557  0.4402371   2.14888511  1.26022696  2.23084828  2.36420824\n",
      "   0.52140326  0.52060672  3.18457493  0.84303611]\n",
      " [ 1.01864829  0.06110217  0.77984864  2.66773852  1.56294279  8.20755867\n",
      "   1.25060923  0.53235177  1.94635966  0.62382406]\n",
      " [ 0.17574565  0.24523189  2.24841683  7.75836334  0.5664361   0.59845291\n",
      "   0.99490567  0.22322618  0.5700707   0.55564894]\n",
      " [ 0.49598399  0.16933718  1.80857656  1.70173195  1.45514798  1.51436973\n",
      "   1.24839075  0.24169216  0.84597385  0.8877212 ]\n",
      " [ 1.01177003  0.44899273  1.25565739  1.98838817  0.98458499  4.2713058\n",
      "   1.09135315  0.28552531  1.68205896  2.04199796]\n",
      " [ 0.3508335   0.61704558  1.26498403  3.26476538  0.26396108  1.03106744\n",
      "   0.29591018  0.29261165  1.52831971  0.70434624]]\n",
      "Probabilities are:\n",
      "[[ 0.00482406  0.00111695  0.01051006  0.01708671  0.02742023  0.02758167\n",
      "   0.00789089  0.00432547  0.02118406  0.00351709]\n",
      " [ 0.0051325   0.00272478  0.00671018  0.0078196   0.0169361   0.01193908\n",
      "   0.00536829  0.00142449  0.00649255  0.01024156]\n",
      " [ 0.00081934  0.00138053  0.00405878  0.02416572  0.00146891  0.00112883\n",
      "   0.01290371  0.00289718  0.01774415  0.00241732]\n",
      " [ 0.00918675  0.00191411  0.00736524  0.00953299  0.00329464  0.00219877\n",
      "   0.00154372  0.00214172  0.02851689  0.01269503]\n",
      " [ 0.00875442  0.00349183  0.01704431  0.00999574  0.01769442  0.01875219\n",
      "   0.00413561  0.0041293   0.02525909  0.00668671]\n",
      " [ 0.00807961  0.00048464  0.00618552  0.0211597   0.01239679  0.06509988\n",
      "   0.00991946  0.00422245  0.01543794  0.00494798]\n",
      " [ 0.00139396  0.00194511  0.01783376  0.061537    0.0044928   0.00474675\n",
      "   0.00789129  0.00177056  0.00452163  0.00440724]\n",
      " [ 0.003934    0.00134313  0.01434508  0.01349763  0.0115418   0.01201152\n",
      "   0.00990186  0.00191703  0.00671001  0.00704114]\n",
      " [ 0.00802506  0.00356128  0.0099595   0.0157713   0.00780943  0.03387871\n",
      "   0.00865628  0.0022647   0.01334158  0.01619651]\n",
      " [ 0.00278271  0.00489422  0.01003347  0.02589514  0.00209366  0.00817812\n",
      "   0.00234707  0.00232091  0.01212217  0.00558666]]\n",
      "True values y are:\n",
      "[[0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]]\n",
      "Probabilities are:\n",
      "[[ 0.00482406  0.00111695  0.01051006  0.01708671  0.02742023  0.02758167\n",
      "   0.00789089  0.00432547  0.02118406  0.00351709]\n",
      " [ 0.0051325   0.00272478  0.00671018  0.0078196   0.0169361   0.01193908\n",
      "   0.00536829  0.00142449  0.00649255  0.01024156]\n",
      " [ 0.00081934  0.00138053  0.00405878  0.02416572  0.00146891  0.00112883\n",
      "   0.01290371  0.00289718  0.01774415  0.00241732]\n",
      " [ 0.00918675  0.00191411  0.00736524  0.00953299  0.00329464  0.00219877\n",
      "   0.00154372  0.00214172  0.02851689  0.01269503]\n",
      " [ 0.00875442  0.00349183  0.01704431  0.00999574  0.01769442  0.01875219\n",
      "   0.00413561  0.0041293   0.02525909  0.00668671]\n",
      " [ 0.00807961  0.00048464  0.00618552  0.0211597   0.01239679  0.06509988\n",
      "   0.00991946  0.00422245  0.01543794  0.00494798]\n",
      " [ 0.00139396  0.00194511  0.01783376  0.061537    0.0044928   0.00474675\n",
      "   0.00789129  0.00177056  0.00452163  0.00440724]\n",
      " [ 0.003934    0.00134313  0.01434508  0.01349763  0.0115418   0.01201152\n",
      "   0.00990186  0.00191703  0.00671001  0.00704114]\n",
      " [ 0.00802506  0.00356128  0.0099595   0.0157713   0.00780943  0.03387871\n",
      "   0.00865628  0.0022647   0.01334158  0.01619651]\n",
      " [ 0.00278271  0.00489422  0.01003347  0.02589514  0.00209366  0.00817812\n",
      "   0.00234707  0.00232091  0.01212217  0.00558666]]\n",
      "True value y max index are:\n",
      "[2 1 4 6 3 8 4 1 0 7]\n",
      "(10,)\n",
      "Probabilities max index are:\n",
      "[5 4 3 8 8 5 3 2 5 3]\n",
      "(10,)\n",
      "Log probabilities are:\n",
      "[[-5.33414016 -6.79715543 -4.55542221 -4.06945416 -3.59647428 -3.59060371\n",
      "  -4.84204588 -5.44323479 -3.85450642 -5.65012028]\n",
      " [-5.27216299 -5.90536749 -5.00413015 -4.8511219  -4.07830757 -4.42793784\n",
      "  -5.22724578 -6.55394067 -5.03709944 -4.58130113]\n",
      " [-7.10701389 -6.58529056 -5.50687377 -3.72282004 -6.52323598 -6.786578\n",
      "  -4.35024021 -5.84401797 -4.03169943 -6.02509429]\n",
      " [-4.68999321 -6.25850266 -4.91098358 -4.65299729 -5.71545705 -6.11985939\n",
      "  -6.47356265 -6.1461448  -3.5572588  -4.36654476]\n",
      " [-4.73819671 -5.65732989 -4.0719389  -4.60559622 -4.03450614 -3.97644487\n",
      "  -5.48811957 -5.48964843 -3.67856923 -5.00763353]\n",
      " [-4.81841151 -7.6320959  -5.08554348 -3.85565693 -4.3903176  -2.73183253\n",
      "  -4.61325724 -5.46733883 -4.17092726 -5.30877495]\n",
      " [-6.57560555 -6.24243908 -4.02666171 -2.78811665 -5.40527905 -5.35029548\n",
      "  -4.8419954  -6.33645783 -5.39888294 -5.42450663]\n",
      " [-5.53809968 -6.61275146 -4.24434795 -4.30524153 -4.46178045 -4.42188872\n",
      "  -4.61503273 -6.25697848 -5.00415488 -4.9559856 ]\n",
      " [-4.82518675 -5.63763664 -4.6092288  -4.14956371 -4.8524231  -3.38496846\n",
      "  -4.7494697  -6.09031264 -4.31686943 -4.12295933]\n",
      " [-5.88433159 -5.31970043 -4.60182856 -3.65370015 -6.16884167 -4.80629344\n",
      "  -6.05458737 -6.06579702 -4.41271915 -5.18737327]]\n",
      "Cross entropy is:\n",
      "[[-0.         -0.         -4.55542221 -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -5.90536749 -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -6.52323598 -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -6.47356265 -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -4.60559622 -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -4.17092726 -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -5.40527905 -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -6.61275146 -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-4.82518675 -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -6.06579702 -0.         -0.        ]]\n",
      "Cross entropy sum is\n",
      "55.1431260927\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "X_batch, y_batch = resample(X_train_normalize, y_train_one_hot, n_samples = batch_size)\n",
    "X.value = X_batch\n",
    "y.value = y_batch\n",
    "print(X_batch.shape)\n",
    "print(y_batch.shape)\n",
    "forward_pass(graph, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of cross entropy with respect to y layer is:\n",
      "[[ -207.29443299  -895.29692917   -95.14691861   -58.5250087    -36.46942642\n",
      "    -36.25595753  -126.72835779  -231.18882213   -47.20531138\n",
      "   -284.32566263]\n",
      " [ -194.83693738  -367.0020688   -149.02739591  -127.88378232\n",
      "    -59.04545482   -83.75851536  -186.27904295  -702.00509774  -154.0226144\n",
      "    -97.64135604]\n",
      " [-1220.49755986  -724.36148901  -246.37968114   -41.38092575\n",
      "   -680.77781053  -885.87689545   -77.49707611  -345.16341469\n",
      "    -56.35660406  -413.68064632]\n",
      " [ -108.85244042  -522.43608839  -135.7728923   -104.89892734\n",
      "   -303.52289661  -454.80073863  -647.78746054  -466.91386684\n",
      "    -35.06693978   -78.77098815]\n",
      " [ -114.22802989  -286.38294672   -58.67060862  -100.04261274\n",
      "    -56.51500301   -53.32711183  -241.80208708  -242.17205076\n",
      "    -39.58970978  -149.55041105]\n",
      " [ -123.76832926 -2063.37011426  -161.66777888   -47.25965315   -80.6660345\n",
      "    -15.36101079  -100.81198387  -236.82911036   -64.77548814\n",
      "   -202.10249196]\n",
      " [ -717.37990579  -514.11093921   -56.07340941   -16.25038578\n",
      "   -222.57832246  -210.67053793  -126.72196049  -564.79217593\n",
      "   -221.15923087  -226.89937383]\n",
      " [ -254.19449038  -744.52874473   -69.71029058   -74.08710727\n",
      "    -86.64163277   -83.25337907  -100.99113408  -521.6404121   -149.03108079\n",
      "   -142.02251522]\n",
      " [ -124.60973744  -280.79830617  -100.40668622   -63.40633043\n",
      "   -128.05029325   -29.51706177  -115.52300688  -441.55943828\n",
      "    -74.95361345   -61.7416862 ]\n",
      " [ -359.36248504  -204.32266423   -99.66639475   -38.61729177\n",
      "   -477.63253015  -122.27754701  -426.06306119  -430.8659481    -82.49347061\n",
      "   -178.99775618]]\n",
      "Gradients of cross entropy with respect to softmax layer is:\n",
      "[[   0.            0.          -95.14691861    0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.         -367.0020688     0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.         -680.77781053\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "  -647.78746054    0.            0.            0.        ]\n",
      " [   0.            0.            0.         -100.04261274    0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "     0.            0.          -64.77548814    0.        ]\n",
      " [   0.            0.            0.            0.         -222.57832246\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.         -744.52874473    0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [-124.60973744    0.            0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "     0.         -430.8659481     0.            0.        ]]\n",
      "grad_cost is:\n",
      "[[   0.            0.          -95.14691861    0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.         -367.0020688     0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.         -680.77781053\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "  -647.78746054    0.            0.            0.        ]\n",
      " [   0.            0.            0.         -100.04261274    0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "     0.            0.          -64.77548814    0.        ]\n",
      " [   0.            0.            0.            0.         -222.57832246\n",
      "     0.            0.            0.            0.            0.        ]\n",
      " [   0.         -744.52874473    0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [-124.60973744    0.            0.            0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.            0.\n",
      "     0.         -430.8659481     0.            0.        ]]\n",
      "The Jacobian is:\n",
      "[[  4.80078471e-03  -1.22767244e-04  -5.44457645e-04  -8.25911276e-04\n",
      "   -6.25902738e-04  -1.25410938e-03  -3.32675716e-04  -1.55134538e-04\n",
      "   -9.31392724e-04  -4.66021395e-04]\n",
      " [ -1.22767244e-04   1.11570030e-03  -2.50768055e-04  -4.57886027e-04\n",
      "   -2.15201681e-04  -3.52178295e-04  -1.34421522e-04  -5.87200556e-05\n",
      "   -3.40764377e-04  -1.88270429e-04]\n",
      " [ -5.44457645e-04  -2.50768055e-04   1.03996005e-02  -2.40955687e-03\n",
      "   -1.15480637e-03  -1.78949805e-03  -7.07080739e-04  -2.83969702e-04\n",
      "   -1.50567692e-03  -7.50543190e-04]\n",
      " [ -8.25911276e-04  -4.57886027e-04  -2.40955687e-03   1.67947568e-02\n",
      "   -1.71668206e-03  -3.37812685e-03  -1.57113885e-03  -5.36745578e-04\n",
      "   -2.58566781e-03  -1.25750919e-03]\n",
      " [ -6.25902738e-04  -2.15201681e-04  -1.15480637e-03  -1.71668206e-03\n",
      "    2.66683594e-02  -2.54789477e-03  -7.49728952e-04  -3.32078891e-04\n",
      "   -1.67650374e-03  -7.34174987e-04]\n",
      " [ -1.25410938e-03  -3.52178295e-04  -1.78949805e-03  -3.37812685e-03\n",
      "   -2.54789477e-03   2.68209252e-02  -1.49185680e-03  -6.23741572e-04\n",
      "   -2.87640273e-03  -1.39732814e-03]\n",
      " [ -3.32675716e-04  -1.34421522e-04  -7.07080739e-04  -1.57113885e-03\n",
      "   -7.49728952e-04  -1.49185680e-03   7.82862750e-03  -1.99436491e-04\n",
      "   -9.78663426e-04  -4.68070694e-04]\n",
      " [ -1.55134538e-04  -5.87200556e-05  -2.83969702e-04  -5.36745578e-04\n",
      "   -3.32078891e-04  -6.23741572e-04  -1.99436491e-04   4.30675896e-03\n",
      "   -4.62069269e-04  -1.83446517e-04]\n",
      " [ -9.31392724e-04  -3.40764377e-04  -1.50567692e-03  -2.58566781e-03\n",
      "   -1.67650374e-03  -2.87640273e-03  -9.78663426e-04  -4.62069269e-04\n",
      "    2.07352926e-02  -1.14218675e-03]\n",
      " [ -4.66021395e-04  -1.88270429e-04  -7.50543190e-04  -1.25750919e-03\n",
      "   -7.34174987e-04  -1.39732814e-03  -4.68070694e-04  -1.83446517e-04\n",
      "   -1.14218675e-03   3.50472376e-03]]\n",
      "The derivative of the cost with respect to the inputs of the softmax layer is:\n",
      "[[  5.18034673e-02   2.38598077e-02  -9.89489938e-01   2.29261911e-01\n",
      "    1.09876268e-01   1.70265225e-01   6.72765535e-02   2.70188421e-02\n",
      "    1.43260519e-01   7.14118718e-02]\n",
      " [  4.50558327e-02  -4.09464318e-01   9.20323950e-02   1.68045119e-01\n",
      "    7.89794620e-02   1.29250163e-01   4.93329768e-02   2.15503819e-02\n",
      "    1.25061231e-01   6.90956370e-02]\n",
      " [  4.26100695e-01   1.46504529e-01   7.86166552e-01   1.16867905e+00\n",
      "   -1.81552273e+01   1.73455022e+00   5.10398834e-01   2.26071940e-01\n",
      "    1.14132654e+00   4.99810040e-01]\n",
      " [  2.15503157e-01   8.70765767e-02   4.58038036e-01   1.01776405e+00\n",
      "    4.85665014e-01   9.66406130e-01  -5.07128672e+00   1.29192458e-01\n",
      "    6.33965895e-01   3.03210326e-01]\n",
      " [  8.26263220e-02   4.58081145e-02   2.41058365e-01  -1.68019135e+00\n",
      "    1.71741358e-01   3.37956636e-01   1.57180836e-01   5.36974300e-02\n",
      "    2.58676964e-01   1.25804504e-01]\n",
      " [  6.03314183e-02   2.20731789e-02   9.75309574e-02   1.67487895e-01\n",
      "    1.08596348e-01   1.86320391e-01   6.33934011e-02   2.99307625e-02\n",
      "   -1.34313870e+00   7.39857044e-02]\n",
      " [  1.39312381e-01   4.78992291e-02   2.57034865e-01   3.82096213e-01\n",
      "   -5.93579870e+00   5.67106143e-01   1.66873412e-01   7.39135624e-02\n",
      "    3.73153390e-01   1.63411437e-01]\n",
      " [  9.14037424e-02  -8.30670944e-01   1.86704025e-01   3.40909309e-01\n",
      "    1.60223837e-01   2.62206864e-01   1.00080687e-01   4.37187693e-02\n",
      "    2.53708874e-01   1.40172746e-01]\n",
      " [ -5.98224522e-01   1.52979941e-02   6.78447242e-02   1.02916587e-01\n",
      "    7.79935758e-02   1.56274241e-01   4.14546336e-02   1.93312740e-02\n",
      "    1.16060603e-01   5.80708037e-02]\n",
      " [  6.68421896e-02   2.53004724e-02   1.22352875e-01   2.31265392e-01\n",
      "    1.43081486e-01   2.68749004e-01   8.59303926e-02  -1.85563578e+00\n",
      "    1.99089914e-01   7.90408575e-02]]\n",
      "grad_cost is:\n",
      "[[  5.18034673e-02   2.38598077e-02  -9.89489938e-01   2.29261911e-01\n",
      "    1.09876268e-01   1.70265225e-01   6.72765535e-02   2.70188421e-02\n",
      "    1.43260519e-01   7.14118718e-02]\n",
      " [  4.50558327e-02  -4.09464318e-01   9.20323950e-02   1.68045119e-01\n",
      "    7.89794620e-02   1.29250163e-01   4.93329768e-02   2.15503819e-02\n",
      "    1.25061231e-01   6.90956370e-02]\n",
      " [  4.26100695e-01   1.46504529e-01   7.86166552e-01   1.16867905e+00\n",
      "   -1.81552273e+01   1.73455022e+00   5.10398834e-01   2.26071940e-01\n",
      "    1.14132654e+00   4.99810040e-01]\n",
      " [  2.15503157e-01   8.70765767e-02   4.58038036e-01   1.01776405e+00\n",
      "    4.85665014e-01   9.66406130e-01  -5.07128672e+00   1.29192458e-01\n",
      "    6.33965895e-01   3.03210326e-01]\n",
      " [  8.26263220e-02   4.58081145e-02   2.41058365e-01  -1.68019135e+00\n",
      "    1.71741358e-01   3.37956636e-01   1.57180836e-01   5.36974300e-02\n",
      "    2.58676964e-01   1.25804504e-01]\n",
      " [  6.03314183e-02   2.20731789e-02   9.75309574e-02   1.67487895e-01\n",
      "    1.08596348e-01   1.86320391e-01   6.33934011e-02   2.99307625e-02\n",
      "   -1.34313870e+00   7.39857044e-02]\n",
      " [  1.39312381e-01   4.78992291e-02   2.57034865e-01   3.82096213e-01\n",
      "   -5.93579870e+00   5.67106143e-01   1.66873412e-01   7.39135624e-02\n",
      "    3.73153390e-01   1.63411437e-01]\n",
      " [  9.14037424e-02  -8.30670944e-01   1.86704025e-01   3.40909309e-01\n",
      "    1.60223837e-01   2.62206864e-01   1.00080687e-01   4.37187693e-02\n",
      "    2.53708874e-01   1.40172746e-01]\n",
      " [ -5.98224522e-01   1.52979941e-02   6.78447242e-02   1.02916587e-01\n",
      "    7.79935758e-02   1.56274241e-01   4.14546336e-02   1.93312740e-02\n",
      "    1.16060603e-01   5.80708037e-02]\n",
      " [  6.68421896e-02   2.53004724e-02   1.22352875e-01   2.31265392e-01\n",
      "    1.43081486e-01   2.68749004e-01   8.59303926e-02  -1.85563578e+00\n",
      "    1.99089914e-01   7.90408575e-02]]\n",
      "The derivatives of the cost with respect to the inputs are:\n",
      "[[ 0.01178714  0.0159466   0.05904617 ..., -0.01055612 -0.0789138\n",
      "   0.22603845]\n",
      " [-0.03368252  0.10173145  0.02923348 ..., -0.0664156  -0.01442221\n",
      "   0.01497851]\n",
      " [ 0.03446034  3.14619343  0.35395781 ..., -2.04844654 -1.62893574\n",
      "  -1.31760397]\n",
      " ..., \n",
      " [-0.06833096  0.20638028  0.05930529 ..., -0.13473581 -0.02925801\n",
      "   0.03038656]\n",
      " [ 0.06551219  0.0172519  -0.01234052 ...,  0.08636845 -0.10371116\n",
      "   0.05543973]\n",
      " [-0.70105011  0.07087194 -0.30457284 ...,  0.36324313 -0.17326175\n",
      "  -0.10074833]]\n",
      "The derivatives of the cost with respect to the weights are:\n",
      "[[-0.29037734  0.41315768 -0.65963643 ...,  0.61560518 -0.95058262\n",
      "  -0.79200696]\n",
      " [-0.29037734  0.41315768 -0.65963643 ...,  0.61560518 -0.95058262\n",
      "  -0.79200696]\n",
      " [-0.29037734  0.41315768 -0.65963643 ...,  0.61560518 -0.95058262\n",
      "  -0.79200696]\n",
      " ..., \n",
      " [-0.29037734  0.41315768 -0.65963643 ...,  0.61560518 -0.95058262\n",
      "  -0.79200696]\n",
      " [-0.29037734  0.41315768 -0.65963643 ...,  0.61560518 -0.95058262\n",
      "  -0.79200696]\n",
      " [-0.29037734  0.41315768 -0.65963643 ...,  0.61560518 -0.95058262\n",
      "  -0.79200696]]\n",
      "The derivatives of the cost with respect to the biases are:\n",
      "[  0.58075468  -0.82631536   1.31927286   2.12823418 -22.75486869\n",
      "   4.77908502  -3.829365    -1.23121036   1.90116523   1.58401393]\n"
     ]
    }
   ],
   "source": [
    "backward_pass(graph, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n",
      "//anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "//anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/ipykernel/__main__.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "//anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "//anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/ipykernel/__main__.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "//anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/ipykernel/__main__.py:45: RuntimeWarning: divide by zero encountered in true_divide\n",
      "//anaconda/envs/CarND-Traffic-Sign-Classifier-Project/lib/python3.5/site-packages/ipykernel/__main__.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of validation set in epoch 1 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 11 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 21 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 31 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 41 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 51 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 61 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 71 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 81 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 91 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 101 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 111 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 121 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 131 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 141 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 151 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 161 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 171 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 181 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 191 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 201 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 211 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 221 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 231 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 241 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 251 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 261 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 271 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 281 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 291 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 301 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 311 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 321 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 331 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 341 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 351 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 361 is: nan\n",
      "0.100583333333\n",
      "Loss of validation set in epoch 371 is: nan\n",
      "0.100583333333\n"
     ]
    }
   ],
   "source": [
    "# Now lets run the model\n",
    "from sklearn.utils import shuffle\n",
    "epochs = 500\n",
    "steps_per_epoch = 100\n",
    "batch_size = 2\n",
    "show_per_step = 10\n",
    "trainables = [W1, b1]\n",
    "accuracies = []\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    X_train_normalize, y_train_one_hot = shuffle(X_train_normalize, y_train_one_hot)\n",
    "    # First shuffle the data\n",
    "    for offset in range(0, X_train_normalize.shape[0], batch_size):\n",
    "       \n",
    "        # Define the ending points\n",
    "        end = offset + batch_size\n",
    "        batch_x, batch_y = X_train_normalize[offset:end], y_train_one_hot[offset:end]\n",
    "        \n",
    "        # Reset the values of X and y \n",
    "        X.value = batch_x\n",
    "        y.value = batch_y\n",
    "#         print(X.value)\n",
    "#         print(y.value)\n",
    "        \n",
    "        # Now run the forward and backward propagation\n",
    "#         if (end % (batch_size*100) == 0):\n",
    "#             print(\"offset is:\")\n",
    "#             print(offset)\n",
    "#             forward_pass(graph, debug = True) \n",
    "#             backward_pass(graph, debug = True)\n",
    "#             print(\"Loss is {0}\".format(graph[-1].value))\n",
    "#         else:\n",
    "#             forward_pass(graph)\n",
    "#             backward_pass(graph)\n",
    "#             print(\"Loss is{0}\".format(graph[-1].value))\n",
    "        \n",
    "        forward_pass(graph)\n",
    "        backward_pass(graph)\n",
    "        \n",
    "        # Update the weights of or biases and weights\n",
    "        sgd_update(trainables, learning_rate = 1e-3) \n",
    "        loss += graph[-1].value\n",
    "\n",
    "#     print(\"Epoch: {}, Loss {:.3f}\".format(i + 1, loss/steps_per_epoch))\n",
    "#     print(\"Average loss per sample is: {0}\".format(loss/(steps_per_epoch * batch_size)))\n",
    "    \n",
    "    # Use the validation set to see our accuracy\n",
    "    X.value = X_validation_normalize\n",
    "    y.value = y_validation_one_hot \n",
    "    forward_pass(graph)\n",
    "    accuracies.append(graph[-1].accuracy)\n",
    "    if (i%show_per_step == 0):\n",
    "        print(\"Loss of validation set in epoch {0} is: {1}\".format(i + 1, graph[-1].value)) \n",
    "        print(graph[-1].accuracy)\n",
    " \n",
    "\n",
    "# # Test it on the test set\n",
    "# X.value = X_test\n",
    "# y.value = y_test\n",
    "# forward_pass(graph)\n",
    "# print(\"Loss of test set is: {0}\".format(graph[-1].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
