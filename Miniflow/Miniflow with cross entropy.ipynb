{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating a neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for layers in the network\n",
    "    \n",
    "    Arguments:\n",
    "        `inbound_layers`: A list of layers with edges into this class\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_layers = []):\n",
    "        \n",
    "        # The list of layers with edges into the class\n",
    "        self.inbound_layers = inbound_layers\n",
    "        \n",
    "        # The value of this layer which is calculated during the forward pass\n",
    "        self.value = None\n",
    "        \n",
    "        # The layers that the this layer outputs to\n",
    "        self.outbound_layers = []\n",
    "        \n",
    "        # The gradients for this layer\n",
    "        # The keys are the input to this layer and their values are the \n",
    "        # partials of this layer with respect to that layer \n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Sets this layer as an outbound layer for all of this layer's inputs\n",
    "        for layer in inbound_layers: \n",
    "            layer.outbound_layers.append(self)\n",
    "        \n",
    "    def forward(debug = False):\n",
    "        # Abstract method that should be implemented for all the derived classes\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward():\n",
    "        # Abstract method that should be implemented for all the derived classes \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    This layer accepts inputs to the neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Note here that nothing is set because these values are set during\n",
    "        # the topological sort\n",
    "        Layer.__init__(self)\n",
    "        \n",
    "    def forward(self, debug = False):\n",
    "        # Do nothing because nothing is calculated\n",
    "        pass\n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "        # An Input layer has no inputs so the gradient is zero \n",
    "        self.gradients = {self : 0}\n",
    "        \n",
    "        # Weights and bias may be inputs, so we need to sum the gradients \n",
    "        # from their outbound layers during the backward pass.\n",
    "        \n",
    "        # Remember that the goal is to figure out the total change in the cost function\n",
    "        # with respect to a single parameter, hence the addition\n",
    "  \n",
    "        for n in self.outbound_layers:\n",
    "#             a = self.gradients[self]\n",
    "#             print(a)\n",
    "#             b = n.gradients[self]\n",
    "#             print(b)\n",
    "            self.gradients[self] += n.gradients[self] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, X, W, b):\n",
    "        Layer.__init__(self, [X, W, b])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        X = self.inbound_layers[0].value\n",
    "        W = self.inbound_layers[1].value\n",
    "        b = self.inbound_layers[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "       \n",
    "        if debug:\n",
    "            print(\"Input to layer is:\")\n",
    "            print(X)\n",
    "\n",
    "            print(\"Weights of layer is:\")\n",
    "            print(W)\n",
    "\n",
    "            print(\"Bias of layer is:\")\n",
    "            print(b)\n",
    "            \n",
    "            print(\"XW + b is:\")\n",
    "            print(self.value)\n",
    "            \n",
    "    def backward(self, debug = False):\n",
    "        \n",
    "        # Initialize a partial derivative for each of the inbound_layers,\n",
    "        # remembering here that this dictionary stores the partial derivative of\n",
    "        # this layer with respect to the inbound layers\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        \n",
    "        for n in self.outbound_layers:\n",
    "            # Get the partial derivative for each of the variables in this layer \n",
    "            # with respect to the cost\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            if debug:\n",
    "                print(\"grad_cost is:\") \n",
    "                print(grad_cost)\n",
    "                \n",
    "            \n",
    "            \n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, self.inbound_layers[1].value.T) \n",
    "           \n",
    "            self.gradients[self.inbound_layers[1]] += np.dot(self.inbound_layers[0].value.T, grad_cost)\n",
    "              \n",
    "            self.gradients[self.inbound_layers[2]] += np.sum(grad_cost, axis = 0, keepdims = False)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"The derivatives of the cost with respect to the inputs are:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "            \n",
    "            print(\"The derivatives of the cost with respect to the weights are:\")\n",
    "            print(self.gradients[self.inbound_layers[1]])\n",
    "            \n",
    "            print(\"The derivatives of the cost with respect to the biases are:\")\n",
    "            print(self.gradients[self.inbound_layers[2]])\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self, layer):\n",
    "        Layer.__init__(self, [layer])\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1. + np.exp(-x))\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        \n",
    "        self.value = self._sigmoid(self.inbound_layers[0].value)\n",
    "            \n",
    "        if debug:\n",
    "            print(\"Input to sigmoid layer is:\")\n",
    "            print(self.inbound_layers[0].value)\n",
    "            \n",
    "            print(\"Value after sigmoid activation is:\")\n",
    "            print(self.value)\n",
    "            \n",
    "        \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients = {n : np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        \n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_layers[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "        \n",
    "        if debug:\n",
    "            print(\"The derivatives of the cost with respect to the sigmoid activation is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MSE(Layer):\n",
    "    def __init__(self, y, a):\n",
    "        Layer.__init__(self, [y, a])\n",
    "        \n",
    "    def forward(self, debug = False):\n",
    "        y = self.inbound_layers[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_layers[1].value.reshape(-1, 1) \n",
    "    \n",
    "        # get the number of samples\n",
    "        self.m = self.inbound_layers[0].value.shape[0] \n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    " \n",
    "        if debug:\n",
    "            print(\"True value of y is:\")\n",
    "            print(y)\n",
    "\n",
    "            print(\"Predicted value of y is:\")\n",
    "            print(a)\n",
    "             \n",
    "            print(\"y - a is:\")\n",
    "            print(self.diff)\n",
    "\n",
    "       \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients[self.inbound_layers[0]] = (2/self.m) * self.diff\n",
    "        self.gradients[self.inbound_layers[1]] = (-2/self.m) * self.diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def __init__(self, logits):\n",
    "        Layer.__init__(self, [logits])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        \n",
    "        logits = self.inbound_layers[0].value\n",
    "        logit_max = np.reshape(np.max(self.inbound_layers[0].value, axis = 1), (-1, 1))\n",
    "        self.inbound_layers[0].value -= logit_max\n",
    "        exp_logits = np.exp(self.inbound_layers[0].value)\n",
    "        sum_exp = np.reshape(np.sum(exp_logits, axis = 1), (-1, 1))\n",
    "        self.value = exp_logits/sum_exp\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Logits are:\")\n",
    "            print(self.inbound_layers[0].value) \n",
    "            \n",
    "            print(\"sum_exp:\")\n",
    "            print(sum_exp)\n",
    "            \n",
    "            print(\"Logits max are:\")\n",
    "            print(np.reshape(np.max(self.inbound_layers[0].value, axis = 1), (-1, 1)))\n",
    "            \n",
    "            print(\"After exponents are:\")\n",
    "            exp_logits = np.exp(self.inbound_layers[0].value)\n",
    "            print(exp_logits)\n",
    "            \n",
    "            print(\"Probabilities are:\")\n",
    "            sum_exp = np.sum(exp_logits)  \n",
    "            self.value = exp_logits/sum_exp \n",
    "            print(self.value)\n",
    "             \n",
    "            \n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "         \n",
    "        # Define gradient for inbound layers\n",
    "        self.gradients = {n : np.zeros_like(n.value) for n in self.inbound_layers}\n",
    "        jacobian = self._calc_jacobian(self.value)\n",
    "        for n in self.outbound_layers:\n",
    "            grad_cost = n.gradients[self]\n",
    "            if debug:\n",
    "                print(\"grad_cost is:\")\n",
    "                print(grad_cost)\n",
    "                print(\"The Jacobian is:\")\n",
    "                print(jacobian)\n",
    "            \n",
    "            self.gradients[self.inbound_layers[0]] += np.dot(grad_cost, jacobian)\n",
    "        \n",
    " \n",
    "        if debug: \n",
    "            print(\"The derivative of the cost with respect to the inputs of the softmax layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "       \n",
    "    def _calc_jacobian(self, probs):\n",
    "        \n",
    "        # First calculate the off diagonal derivatives\n",
    "        jacobian = np.dot(-1 * probs.T, probs)\n",
    "        dims = jacobian.shape[0]\n",
    "        \n",
    "        # Now calculate the diagonal derivatives\n",
    "        for i in range(dims):\n",
    "            jacobian[i,i] = probs[0,i] * (1 - probs[0,i])\n",
    "        return(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Layer):\n",
    "    def __init__(self, y, probs):\n",
    "        Layer.__init__(self, [y, probs])\n",
    "    \n",
    "    def forward(self, debug = False):\n",
    "        n_samples_in_batch = self.inbound_layers[0].value.shape[0]\n",
    "        n_classes = self.inbound_layers[0].value.shape[1]\n",
    "        \n",
    "        self.y_flat = self.inbound_layers[0].value.reshape(n_samples_in_batch, n_classes)\n",
    "        self.probs_flat = self.inbound_layers[1].value.reshape(n_samples_in_batch, n_classes)\n",
    "       \n",
    "        # Calculate the accuracy\n",
    "        n_correct = np.sum(np.argmax(self.y_flat, axis = 1) == np.argmax(self.probs_flat, axis = 1))\n",
    "        self.accuracy = n_correct/n_samples_in_batch\n",
    "        \n",
    "        # Calculate the cross entropy\n",
    "        self.log_probs = np.log(self.probs_flat)\n",
    "        self.cross_entropy = self.y_flat * self.log_probs\n",
    "        self.value = -1 * np.sum(self.cross_entropy)\n",
    "       \n",
    "        \n",
    "        if debug:\n",
    "            print(\"True values y are:\")\n",
    "            print(self.y_flat)\n",
    "            print(\"Probabilities are:\")\n",
    "            print(self.probs_flat)\n",
    "            print(\"True value y max index are:\")\n",
    "            print(np.argmax(self.y_flat, axis = 1))\n",
    "            print(np.argmax(self.y_flat, axis = 1).shape)\n",
    "            print(\"Probabilities max index are:\")\n",
    "            print(np.argmax(self.probs_flat, axis = 1))\n",
    "            print(np.argmax(self.probs_flat, axis = 1).shape) \n",
    "            print(\"Log probabilities are:\")\n",
    "            print(self.log_probs)\n",
    "            print(\"Cross entropy is:\")\n",
    "            print(self.cross_entropy)\n",
    "            print(\"Cross entropy sum is\")\n",
    "            print(self.value)\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "    def backward(self, debug = False):\n",
    "        self.gradients[self.inbound_layers[0]] = -1 * 1/self.probs_flat\n",
    "        self.gradients[self.inbound_layers[1]] = -1 * self.y_flat/self.probs_flat\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Gradients of cross entropy with respect to y layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[0]])\n",
    "            print(\"Gradients of cross entropy with respect to softmax layer is:\")\n",
    "            print(self.gradients[self.inbound_layers[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    input_layers = [n for n in feed_dict.keys()]\n",
    "    \n",
    "    G = {}\n",
    "    \n",
    "    layers = [n for n in input_layers]\n",
    "    \n",
    "    # Think of each element in the layer as a node, in this while loop\n",
    "    # we are simply finding which layers are connected to which other layers\n",
    "    while len(layers) > 0:\n",
    "        # Get the first element of the array\n",
    "        n = layers.pop(0)\n",
    "        \n",
    "        # Check if this layer n is in the dictionary if it isn't add it in\n",
    "        if n not in G:\n",
    "            G[n] = {'in' : set(), 'out' : set()}\n",
    "        # Check if this layer m is in the dictionary if it isn't add it in \n",
    "        for m in n.outbound_layers:\n",
    "            if m not in G:\n",
    "                G[m] = {'in' : set(), 'out' : set()}\n",
    "            # Add the edges between the nodes\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            layers.append(m)\n",
    "        \n",
    "    L = []\n",
    "    S = set(input_layers)\n",
    "    \n",
    "    while len(S) > 0:\n",
    "        # Get the last layer \n",
    "        n = S.pop()\n",
    "        \n",
    "        # Check if it is an input layer, if it is then initialize the value\n",
    "        if (isinstance(n, Input)):\n",
    "            n.value = feed_dict[n]\n",
    "            \n",
    "        L.append(n)\n",
    "        for m in n.outbound_layers:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            \n",
    "            # if there are no incoming edges to m then add it to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(graph, debug = False):\n",
    "    for n in graph:\n",
    "        n.forward(debug)\n",
    "\n",
    "def backward_pass(graph, debug = False):\n",
    "    for n in graph[::-1]:\n",
    "        n.backward(debug) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sgd_update(trainable, learning_rate = 1e-2): \n",
    "     \n",
    "    for t in trainable:\n",
    "        partial = t.gradients[t]\n",
    "#         print(\"Partial derivatives are:\")\n",
    "#         print(partial)\n",
    "        t.value -= learning_rate * partial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Creating a neural network to classify numbers from the NMIST data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import helper functions\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Just using keras to import the data\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Use one hot encoding for the y label vector\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "y_train_one_hot = lb.transform(y_train)\n",
    "y_test_one_hot = lb.transform(y_test)\n",
    "print(y_train_one_hot.shape)\n",
    "print(y_test_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/Gf89zG8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e52b5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Print the first image from the training set out\n",
    "X_batch, y_batch = resample(X_train, y_train_one_hot, n_samples = 1)\n",
    "plt.imshow(X_batch[0], cmap = plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Normalize the grayscale image so that the values range between -0.5 and 0.5, \n",
    "# this is so that the sigmoid activation function does not saturate during training \n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [-0.5, 0.5]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    img_min = np.min(image_data)\n",
    "    img_max = np.max(image_data)\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    scaled_img = a + ((image_data - img_min) * (b-a))/(img_max - img_min)\n",
    "    return(scaled_img)\n",
    "\n",
    "X_train_normalize = normalize_grayscale(X_train)\n",
    "X_test_normalize = normalize_grayscale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "[[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      "  -0.42941176 -0.42941176 -0.42941176 -0.00588235  0.03333333  0.18627451\n",
      "  -0.39803922  0.15098039  0.5         0.46862745 -0.00196078 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.38235294 -0.35882353 -0.13137255  0.10392157  0.16666667\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.49215686  0.38235294\n",
      "   0.1745098   0.49215686  0.44901961  0.26470588 -0.24901961 -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.30784314  0.43333333  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.48431373 -0.13529412\n",
      "  -0.17843137 -0.17843137 -0.28039216 -0.34705882 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.42941176  0.35882353  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.27647059  0.21372549  0.46862745  0.44509804 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.18627451  0.11176471 -0.08039216  0.49215686  0.49215686\n",
      "   0.30392157 -0.45686275 -0.5        -0.33137255  0.10392157 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.44509804 -0.49607843  0.10392157  0.49215686\n",
      "  -0.14705882 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5         0.04509804  0.49215686\n",
      "   0.24509804 -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.45686275  0.24509804\n",
      "   0.49215686 -0.2254902  -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.3627451\n",
      "   0.44509804  0.38235294  0.12745098 -0.07647059 -0.49607843 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.18235294  0.44117647  0.49215686  0.49215686 -0.03333333 -0.40196078\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.32352941  0.22941176  0.49215686  0.49215686  0.08823529 -0.39411765\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.4372549  -0.13529412  0.48823529  0.49215686  0.23333333\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5         0.47647059  0.49215686  0.47647059\n",
      "  -0.24901961 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.31960784  0.00980392  0.21764706  0.49215686  0.49215686  0.31176471\n",
      "  -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.34705882\n",
      "   0.08039216  0.39803922  0.49215686  0.49215686  0.49215686  0.48039216\n",
      "   0.21372549 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.40588235 -0.05294118  0.36666667\n",
      "   0.49215686  0.49215686  0.49215686  0.49215686  0.28823529 -0.19411765\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.40980392 -0.24117647  0.33529412  0.49215686  0.49215686\n",
      "   0.49215686  0.49215686  0.27647059 -0.18235294 -0.49215686 -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.42941176  0.17058824  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "   0.49215686  0.26470588 -0.18627451 -0.46470588 -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.28431373  0.1745098\n",
      "   0.38627451  0.49215686  0.49215686  0.49215686  0.49215686  0.45686275\n",
      "   0.02156863 -0.45686275 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5         0.03333333  0.49215686\n",
      "   0.49215686  0.49215686  0.33137255  0.02941176  0.01764706 -0.4372549\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]\n",
      " [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the data and before and after the reshape to see if we did it properly \n",
    "print(X_train[0])\n",
    "print(X_train_normalize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now flatten the array into a 1 dimension array\n",
    "X_train_normalize = np.array(X_train_normalize).reshape(60000, 28 * 28)\n",
    "X_test_normalize = np.array(X_test_normalize).reshape(10000, 28 * 28 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.48823529\n",
      " -0.42941176 -0.42941176 -0.42941176 -0.00588235  0.03333333  0.18627451\n",
      " -0.39803922  0.15098039  0.5         0.46862745 -0.00196078 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.38235294 -0.35882353 -0.13137255\n",
      "  0.10392157  0.16666667  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.38235294  0.1745098   0.49215686  0.44901961  0.26470588\n",
      " -0.24901961 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.30784314\n",
      "  0.43333333  0.49215686  0.49215686  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.49215686  0.48431373 -0.13529412 -0.17843137\n",
      " -0.17843137 -0.28039216 -0.34705882 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.42941176  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.27647059  0.21372549  0.46862745  0.44509804\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.18627451  0.11176471\n",
      " -0.08039216  0.49215686  0.49215686  0.30392157 -0.45686275 -0.5\n",
      " -0.33137255  0.10392157 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.44509804 -0.49607843  0.10392157  0.49215686 -0.14705882 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5         0.04509804  0.49215686  0.24509804 -0.49215686\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.45686275  0.24509804  0.49215686\n",
      " -0.2254902  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.3627451\n",
      "  0.44509804  0.38235294  0.12745098 -0.07647059 -0.49607843 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.18235294  0.44117647  0.49215686  0.49215686 -0.03333333 -0.40196078\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.32352941  0.22941176  0.49215686  0.49215686\n",
      "  0.08823529 -0.39411765 -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.4372549  -0.13529412\n",
      "  0.48823529  0.49215686  0.23333333 -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      "  0.47647059  0.49215686  0.47647059 -0.24901961 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.31960784  0.00980392\n",
      "  0.21764706  0.49215686  0.49215686  0.31176471 -0.49215686 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.34705882  0.08039216  0.39803922\n",
      "  0.49215686  0.49215686  0.49215686  0.48039216  0.21372549 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.40588235 -0.05294118  0.36666667  0.49215686\n",
      "  0.49215686  0.49215686  0.49215686  0.28823529 -0.19411765 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.40980392 -0.24117647  0.33529412  0.49215686  0.49215686\n",
      "  0.49215686  0.49215686  0.27647059 -0.18235294 -0.49215686 -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.42941176  0.17058824  0.35882353  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.26470588 -0.18627451 -0.46470588 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.28431373  0.1745098   0.38627451  0.49215686  0.49215686  0.49215686\n",
      "  0.49215686  0.45686275  0.02156863 -0.45686275 -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5         0.03333333  0.49215686  0.49215686  0.49215686  0.33137255\n",
      "  0.02941176  0.01764706 -0.4372549  -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n",
      " -0.5        -0.5        -0.5        -0.5        -0.5       ]\n"
     ]
    }
   ],
   "source": [
    "# See if we did the reshaping correctly\n",
    "print(X_train_normalize[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(48000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Now define a validation set\n",
    "X_train_normalize, X_validation_normalize, y_train_one_hot, y_validation_one_hot = train_test_split(X_train_normalize, y_train_one_hot, test_size = 0.2)\n",
    "\n",
    "# Check that the shape is correct\n",
    "print(X_train_normalize.shape)\n",
    "print(y_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now lets define the network again!\n",
    "n_hidden = 10\n",
    "# n_hidden_2 = 15\n",
    "n_classes = 10\n",
    "n_features = X_train_normalize.shape[1]\n",
    "\n",
    "# Initialize the weights \n",
    "W1_ = np.random.normal(loc = 0, scale = 0.1, size = (n_features, n_hidden))\n",
    "b1_ = np.zeros(n_hidden)\n",
    "# W2_ = np.random.normal(loc = 0, scale = 0.1, size = (n_hidden, n_hidden_2))\n",
    "# b2_ = np.zeros(n_hidden_2)\n",
    "# W3_ = np.random.normal(loc = 0, scale = 0.1, size = (n_hidden_2, n_classes))\n",
    "# b3_ = np.zeros(n_classes)\n",
    "\n",
    "# W2_ = np.random.normal(loc = 0, scale = 1, size = (n_hidden, n_classes))\n",
    "# b2_ = np.zeros(n_classes)\n",
    "# Build the layers for the neural network\n",
    "X, y, = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "# W3, b3 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "# s1 = Sigmoid(l1)\n",
    "# l2 = Linear(s1, W2, b2)\n",
    "# s2 = Sigmoid(l2)\n",
    "# l3 = Linear(s2, W3, b3)\n",
    "probs = Softmax(l1)\n",
    "cost = CrossEntropy(y, probs)\n",
    "\n",
    "# Define the input layers to the neural network \n",
    "feed_dict = {\n",
    "    X: X_train,\n",
    "    y: y_train_one_hot,\n",
    "    W1: W1_,\n",
    "    b1: b1_\n",
    "#     W2: W2_,\n",
    "#     b2: b2_\n",
    "#     W3: W3_,\n",
    "#     b3: b3_\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 784)\n",
      "(5, 10)\n",
      "Input to layer is:\n",
      "[[-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]\n",
      " [-0.5 -0.5 -0.5 ..., -0.5 -0.5 -0.5]]\n",
      "Weights of layer is:\n",
      "[[-0.06461156  0.0008519  -0.02425677 ...,  0.08459943  0.08628213\n",
      "   0.01494777]\n",
      " [-0.02010472 -0.14911334 -0.02884505 ..., -0.00072358 -0.13153255\n",
      "   0.00868369]\n",
      " [ 0.18851576 -0.07414322 -0.03309469 ..., -0.02557268  0.03838866\n",
      "  -0.06348232]\n",
      " ..., \n",
      " [ 0.01428979 -0.04725193 -0.06415367 ..., -0.15176712  0.0413307\n",
      "   0.06323723]\n",
      " [ 0.11280719  0.02886799 -0.03371613 ..., -0.00248427 -0.01817586\n",
      "   0.10605946]\n",
      " [-0.06143577  0.1108776  -0.15352727 ..., -0.07807271  0.03360923\n",
      "   0.13976584]]\n",
      "Bias of layer is:\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "XW + b is:\n",
      "[[-0.9445978  -1.4383392   1.73475364  0.93642449  1.38890624  0.85819315\n",
      "  -0.07229913  0.80754177 -0.52012355 -1.54362756]\n",
      " [-0.27801429 -0.79580898  0.82173161  1.71252721  3.08609511  0.35990747\n",
      "   0.84479696  2.14270015 -2.52927666 -1.0939683 ]\n",
      " [-0.86278064 -1.03753438  0.33794411  1.22352989  1.79159411 -0.55039835\n",
      "  -0.27519958  0.13384966 -1.97428625  0.05445295]\n",
      " [-0.92578005 -0.93991727  1.04439548  2.25835764  0.19987881  1.06415303\n",
      "   0.50386596 -0.40074106 -1.43593334 -1.39114533]\n",
      " [-0.66915676 -1.34843858 -0.07300388  1.13935373  1.04453987 -0.42031057\n",
      "   0.47727901  0.2424748  -2.37064261 -0.70526491]]\n",
      "Logits are:\n",
      "[[-2.67935144 -3.17309285  0.         -0.79832916 -0.3458474  -0.87656049\n",
      "  -1.80705277 -0.92721188 -2.2548772  -3.27838121]\n",
      " [-3.3641094  -3.88190409 -2.2643635  -1.37356791  0.         -2.72618764\n",
      "  -2.24129815 -0.94339497 -5.61537177 -4.18006341]\n",
      " [-2.65437475 -2.82912849 -1.45365    -0.56806422  0.         -2.34199246\n",
      "  -2.06679369 -1.65774445 -3.76588036 -1.73714116]\n",
      " [-3.18413769 -3.19827492 -1.21396216  0.         -2.05847883 -1.19420461\n",
      "  -1.75449168 -2.6590987  -3.69429098 -3.64950297]\n",
      " [-1.80851048 -2.4877923  -1.21235761  0.         -0.09481386 -1.5596643\n",
      "  -0.66207472 -0.89687893 -3.50999634 -1.84461864]]\n",
      "sum_exp:\n",
      "[[ 3.38676248]\n",
      " [ 1.99233415]\n",
      " [ 2.54220939]\n",
      " [ 2.10373303]\n",
      " [ 3.77583854]]\n",
      "Logits max are:\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "After exponents are:\n",
      "[[ 0.06860764  0.04187389  1.          0.45008035  0.70762046  0.41621202\n",
      "   0.16413717  0.39565531  0.10488642  0.03768922]\n",
      " [ 0.03459281  0.02061154  0.10389614  0.25320195  1.          0.0654684\n",
      "   0.10632039  0.38930392  0.00364146  0.01529754]\n",
      " [ 0.07034281  0.05906431  0.23371567  0.56662123  1.          0.0961359\n",
      "   0.12659102  0.19056833  0.02314723  0.1760229 ]\n",
      " [ 0.04141394  0.04083258  0.29701811  1.          0.127648    0.30294482\n",
      "   0.17299516  0.07001129  0.02486508  0.02600405]\n",
      " [ 0.16389808  0.08309321  0.29749507  1.          0.90954222  0.21020663\n",
      "   0.51578013  0.40784057  0.02989702  0.1580856 ]]\n",
      "Probabilities are:\n",
      "[[ 0.00497125  0.00303415  0.07245916  0.03261244  0.05127358  0.03015837\n",
      "   0.01189324  0.02866885  0.00759998  0.00273093]\n",
      " [ 0.00250657  0.00149349  0.00752823  0.0183468   0.07245916  0.00474379\n",
      "   0.00770389  0.02820863  0.00026386  0.00110845]\n",
      " [ 0.00509698  0.00427975  0.01693484  0.0410569   0.07245916  0.00696593\n",
      "   0.00917268  0.01380842  0.00167723  0.01275447]\n",
      " [ 0.00300082  0.00295869  0.02152168  0.07245916  0.00924927  0.02195113\n",
      "   0.01253508  0.00507296  0.0018017   0.00188423]\n",
      " [ 0.01187592  0.00602086  0.02155624  0.07245916  0.06590467  0.0152314\n",
      "   0.03737299  0.02955179  0.00216631  0.01145475]]\n",
      "True values y are:\n",
      "[[0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]]\n",
      "Probabilities are:\n",
      "[[ 0.00497125  0.00303415  0.07245916  0.03261244  0.05127358  0.03015837\n",
      "   0.01189324  0.02866885  0.00759998  0.00273093]\n",
      " [ 0.00250657  0.00149349  0.00752823  0.0183468   0.07245916  0.00474379\n",
      "   0.00770389  0.02820863  0.00026386  0.00110845]\n",
      " [ 0.00509698  0.00427975  0.01693484  0.0410569   0.07245916  0.00696593\n",
      "   0.00917268  0.01380842  0.00167723  0.01275447]\n",
      " [ 0.00300082  0.00295869  0.02152168  0.07245916  0.00924927  0.02195113\n",
      "   0.01253508  0.00507296  0.0018017   0.00188423]\n",
      " [ 0.01187592  0.00602086  0.02155624  0.07245916  0.06590467  0.0152314\n",
      "   0.03737299  0.02955179  0.00216631  0.01145475]]\n",
      "True value y max index are:\n",
      "[3 5 7 3 7]\n",
      "(5,)\n",
      "Probabilities max index are:\n",
      "[2 4 4 3 3]\n",
      "(5,)\n",
      "Log probabilities are:\n",
      "[[-5.30408363 -5.79782503 -2.62473218 -3.42306134 -2.97057959 -3.50129267\n",
      "  -4.43178495 -3.55194406 -4.87960938 -5.90311339]\n",
      " [-5.98884159 -6.50663628 -4.88909569 -3.99830009 -2.62473218 -5.35091982\n",
      "  -4.86603034 -3.56812715 -8.24010395 -6.80479559]\n",
      " [-5.27910694 -5.45386067 -4.07838218 -3.19279641 -2.62473218 -4.96672465\n",
      "  -4.69152587 -4.28247664 -6.39061254 -4.36187334]\n",
      " [-5.80886987 -5.8230071  -3.83869434 -2.62473218 -4.68321101 -3.8189368\n",
      "  -4.37922386 -5.28383088 -6.31902316 -6.27423515]\n",
      " [-4.43324267 -5.11252449 -3.83708979 -2.62473218 -2.71954604 -4.18439648\n",
      "  -3.2868069  -3.52161111 -6.13472852 -4.46935082]]\n",
      "Cross entropy is:\n",
      "[[-0.         -0.         -0.         -3.42306134 -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -5.35091982\n",
      "  -0.         -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -4.28247664 -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -2.62473218 -0.         -0.         -0.\n",
      "  -0.         -0.         -0.        ]\n",
      " [-0.         -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -3.52161111 -0.         -0.        ]]\n",
      "Cross entropy sum is\n",
      "19.2028010983\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "X_batch, y_batch = resample(X_train_normalize, y_train_one_hot, n_samples = batch_size)\n",
    "X.value = X_batch\n",
    "y.value = y_batch\n",
    "print(X_batch.shape)\n",
    "print(y_batch.shape)\n",
    "forward_pass(graph, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of cross entropy with respect to y layer is:\n",
      "[[ -9.94795831  -9.81294809 -26.00542315 -12.65472164 -15.95281936\n",
      "  -10.39923451  -3.0337511  -45.56715885  -7.40711143 -28.84042258]]\n",
      "Gradients of cross entropy with respect to softmax layer is:\n",
      "[[  0.           0.           0.           0.         -15.95281936   0.\n",
      "    0.           0.           0.           0.        ]]\n",
      "grad_cost is:\n",
      "[[  0.           0.           0.           0.         -15.95281936   0.\n",
      "    0.           0.           0.           0.        ]]\n",
      "The Jacobian is:\n",
      "[[ 0.09041824 -0.01024393 -0.00386547 -0.00794353 -0.00630128 -0.0096664\n",
      "  -0.03313493 -0.00220604 -0.01357117 -0.00348549]\n",
      " [-0.01024393  0.09152131 -0.00391865 -0.00805282 -0.00638797 -0.00979939\n",
      "  -0.03359082 -0.0022364  -0.01375788 -0.00353345]\n",
      " [-0.00386547 -0.00391865  0.03697484 -0.00303867 -0.00241045 -0.00369773\n",
      "  -0.01267524 -0.00084389 -0.00519143 -0.00133332]\n",
      " [-0.00794353 -0.00805282 -0.00303867  0.07277743 -0.00495347 -0.00759882\n",
      "  -0.02604758 -0.00173419 -0.01066838 -0.00273997]\n",
      " [-0.00630128 -0.00638797 -0.00241045 -0.00495347  0.05875545 -0.00602783\n",
      "  -0.02066249 -0.00137566 -0.00846279 -0.00217351]\n",
      " [-0.0096664  -0.00979939 -0.00369773 -0.00759882 -0.00602783  0.086914\n",
      "  -0.03169704 -0.00211031 -0.01298224 -0.00333424]\n",
      " [-0.03313493 -0.03359082 -0.01267524 -0.02604758 -0.02066249 -0.03169704\n",
      "   0.22097234 -0.00723383 -0.04450114 -0.01142927]\n",
      " [-0.00220604 -0.0022364  -0.00084389 -0.00173419 -0.00137566 -0.00211031\n",
      "  -0.00723383  0.02146402 -0.00296278 -0.00076093]\n",
      " [-0.01357117 -0.01375788 -0.00519143 -0.01066838 -0.00846279 -0.01298224\n",
      "  -0.04450114 -0.00296278  0.11677894 -0.00468112]\n",
      " [-0.00348549 -0.00353345 -0.00133332 -0.00273997 -0.00217351 -0.00333424\n",
      "  -0.01142927 -0.00076093 -0.00468112  0.0334713 ]]\n",
      "The derivative of the cost with respect to the inputs of the softmax layer is:\n",
      "[[ 0.10052314  0.10190617  0.03845352  0.07902189 -0.93731516  0.09616092\n",
      "   0.32962493  0.02194563  0.13500539  0.03467356]]\n",
      "grad_cost is:\n",
      "[[ 0.10052314  0.10190617  0.03845352  0.07902189 -0.93731516  0.09616092\n",
      "   0.32962493  0.02194563  0.13500539  0.03467356]]\n",
      "The derivatives of the cost with respect to the inputs are:\n",
      "[[  9.40052954e-02  -4.83685614e-03   3.83759680e-02  -2.50283730e-02\n",
      "   -2.46598767e-01  -5.30982213e-03   9.38936392e-03   4.08782634e-02\n",
      "    4.59661777e-02   1.08789916e-01  -1.00926043e-02  -1.12690255e-01\n",
      "   -1.37310906e-01   2.18101364e-02   1.38664001e-01   1.22949186e-01\n",
      "   -1.72674032e-01   6.93341419e-02   1.31759735e-01  -1.11969143e-01\n",
      "    8.28381488e-02  -1.30898316e-01  -5.13725811e-02   2.04723073e-01\n",
      "   -1.76444764e-01  -3.27136996e-02   3.20631349e-02   4.40041681e-03\n",
      "   -5.68594326e-02   1.27944609e-01  -6.84183548e-02   1.10099512e-01\n",
      "    1.12106292e-01   4.58954472e-03   1.59281583e-01   3.74764820e-02\n",
      "   -3.94479722e-02  -1.93294618e-01   9.66291078e-03  -2.61892088e-01\n",
      "    8.70438331e-02  -1.35660799e-01  -9.56948850e-02  -1.00524790e-01\n",
      "   -5.51841064e-02   1.54159889e-01   3.87067182e-03  -1.31219180e-01\n",
      "   -7.47363695e-02  -1.87041769e-01   7.45003945e-04  -3.54234240e-02\n",
      "   -1.89808690e-01  -1.58874909e-02   8.48686976e-02   7.63592224e-02\n",
      "    2.18572172e-02   7.18075437e-02   2.30758894e-01   1.41020331e-01\n",
      "    2.21161748e-02  -1.85624030e-02   8.24498713e-02  -1.45324881e-01\n",
      "    1.09409358e-01  -5.12380883e-02   1.38831495e-01   2.17301829e-01\n",
      "    1.09559309e-02   2.85436067e-02  -2.24956665e-01   7.83208645e-02\n",
      "    4.53889530e-02  -3.69759216e-02   1.49283713e-01  -6.08636033e-02\n",
      "    1.21474482e-01   3.93994357e-02   2.06600161e-02   1.56980392e-01\n",
      "   -4.95951592e-02  -1.59273383e-01   1.78433301e-01   4.44353075e-02\n",
      "   -1.16720803e-01  -1.34808147e-01  -5.10397260e-02  -2.58451082e-03\n",
      "   -4.04080404e-02  -7.73400092e-02   5.23240991e-02   6.68392964e-02\n",
      "    6.84919158e-02   6.18734594e-02   1.52661654e-01   2.26488975e-02\n",
      "    1.32552540e-01   2.09919979e-01   1.30585488e-02   4.01074927e-02\n",
      "   -2.19022386e-03  -1.86943482e-01  -1.10675289e-01   1.46994186e-01\n",
      "   -1.82635294e-01   4.87238045e-02  -7.30081036e-03   1.77782100e-01\n",
      "   -3.14565209e-03  -5.92601611e-02  -1.04039610e-01   7.18160278e-03\n",
      "    2.93508359e-02  -1.23414050e-01  -7.58679198e-02  -5.31523774e-02\n",
      "   -5.13156805e-02   3.61895541e-03   6.82400984e-03   2.16371819e-02\n",
      "    1.00772568e-01  -8.89431134e-02  -2.06220766e-01  -1.78714764e-01\n",
      "    3.41004673e-02   3.38959424e-02  -6.30246490e-02   4.39398944e-02\n",
      "   -7.49053596e-02  -8.40445898e-02   7.50877696e-02  -2.47069878e-01\n",
      "   -3.28835224e-02  -3.07543247e-03   1.07079662e-01   4.77316090e-02\n",
      "    9.87319210e-02   6.95036889e-02   9.58743172e-03  -9.81355035e-02\n",
      "    2.37378065e-02   6.66326045e-02  -1.21728933e-01   1.35677959e-01\n",
      "    1.95505774e-01   8.01448035e-02   7.65835157e-02  -5.16546315e-02\n",
      "    4.01051859e-02  -3.54303236e-02  -9.56998286e-02  -6.10436988e-02\n",
      "   -3.43961600e-02  -9.71988879e-03   4.83036939e-02  -7.62525592e-02\n",
      "   -3.75971252e-02   4.65402477e-02  -2.59725099e-02   1.23911546e-02\n",
      "   -5.55833700e-02  -2.89114598e-02  -1.55795762e-01  -2.31274418e-01\n",
      "    6.16454570e-02   9.32776860e-02   1.76296407e-01   1.32908017e-01\n",
      "    1.75153825e-03  -1.07185985e-01   2.25050219e-03  -1.09667232e-01\n",
      "   -5.65838624e-02  -1.30133868e-02  -1.16887175e-01   1.36407745e-01\n",
      "    7.48587091e-02   1.70587818e-01  -6.72467570e-02  -4.07653697e-02\n",
      "    7.25843599e-02   1.53067120e-02  -4.94705323e-02  -1.22222551e-01\n",
      "    5.65149522e-02  -1.41437648e-01   2.54489717e-02   4.50507851e-02\n",
      "   -6.07843141e-02  -7.88114949e-02   1.32916868e-01   8.44969073e-02\n",
      "    3.11179177e-02   3.64200966e-02   1.89421229e-02  -9.06909606e-02\n",
      "   -1.21441040e-01   5.71446413e-02  -4.93099315e-02  -6.31401030e-02\n",
      "   -9.70833060e-03   2.12452772e-02  -2.74961601e-02   2.14556249e-02\n",
      "    8.75960025e-02  -5.85206947e-02  -5.01339843e-02   9.61587087e-02\n",
      "   -8.14582428e-02   1.02320839e-01   5.17911223e-02  -1.28054973e-01\n",
      "    2.27704060e-01   6.21396687e-02  -2.04219494e-02   5.89323586e-02\n",
      "   -2.23027104e-02  -4.28784291e-02   3.21323997e-02  -1.61159832e-01\n",
      "    8.50796310e-02  -1.72605888e-01   8.96320923e-02  -2.65166224e-02\n",
      "   -3.99934557e-02   1.35306605e-01   1.88087399e-01   6.26094000e-02\n",
      "   -9.70363194e-03   1.49191957e-01   7.58466437e-02   2.83994187e-02\n",
      "   -1.44828672e-01   5.71519300e-02   3.99714816e-02   3.66533066e-02\n",
      "    4.39789286e-02   1.96744028e-02   3.30878799e-02   7.67546211e-02\n",
      "    5.05823057e-02  -1.24464063e-01   2.80963678e-03   1.24372139e-01\n",
      "    1.20495585e-02   3.28843260e-02   1.08305208e-01  -5.35799455e-02\n",
      "   -2.36419773e-01   4.20277549e-02   5.70522629e-02   9.64380662e-02\n",
      "    4.82135744e-02   7.42165222e-02  -5.71407737e-02  -7.55268446e-02\n",
      "    8.78968062e-02  -8.15975833e-02  -2.36251210e-02  -1.28726693e-01\n",
      "   -7.46673736e-02   4.92538897e-02  -5.21377672e-02   1.07558159e-01\n",
      "    3.76262578e-02  -2.07522812e-02   6.04362560e-03   8.21527146e-02\n",
      "    1.46580554e-01   2.68892925e-02   1.05699759e-01   2.41751934e-01\n",
      "   -8.78457709e-02  -5.00529648e-02   9.14270852e-03   8.45776405e-02\n",
      "    4.46946183e-02   1.24057181e-01  -9.05962846e-02  -1.20938941e-01\n",
      "   -2.74410710e-02   7.27493367e-02  -6.44822010e-02   8.55437372e-02\n",
      "   -2.14424321e-02  -3.22262373e-02   1.73379133e-01   4.62663646e-03\n",
      "    7.49028428e-03   1.06369523e-01   6.95047769e-02   1.71179569e-01\n",
      "   -1.29940596e-02  -1.62198313e-01   1.34641703e-01  -8.35438604e-02\n",
      "   -5.80191397e-02  -7.49171442e-02   3.65158351e-02  -3.60110478e-02\n",
      "    3.68152709e-02  -1.03836639e-01  -3.56211350e-02   1.03924964e-01\n",
      "   -1.67471286e-02  -2.94346504e-02  -8.00381819e-02   2.96627617e-02\n",
      "   -5.39243754e-03   9.46074514e-02  -6.00546489e-02  -1.33005496e-01\n",
      "   -2.97434654e-02   1.84250971e-01   5.96951865e-02   1.49196818e-01\n",
      "    5.84682147e-02   6.72120423e-02  -3.89328284e-02   1.87667903e-01\n",
      "    8.24749949e-02  -2.99119687e-02   1.01491963e-01   7.02951909e-02\n",
      "    1.25756224e-02  -1.36973994e-01  -1.20134194e-01  -8.75154209e-02\n",
      "    1.54071991e-02  -8.22817462e-02  -9.85920025e-02   1.04213323e-01\n",
      "   -6.29220980e-02  -1.43939799e-01   4.02643869e-02   4.34275594e-02\n",
      "    5.68294864e-02  -1.95946858e-02  -3.54924568e-02   1.05129184e-01\n",
      "    9.10486047e-02  -3.12285651e-03  -4.17786734e-02  -1.19449974e-01\n",
      "   -8.44598936e-03   1.24119627e-01  -1.20895443e-01   1.87584354e-02\n",
      "    2.02035030e-02   2.03698355e-01  -1.48738954e-01  -7.12289254e-02\n",
      "   -1.88325830e-03   1.31471145e-01   3.12066687e-03  -4.41486891e-02\n",
      "   -1.76329630e-01  -9.08972435e-02   7.77249720e-03   3.05906830e-01\n",
      "    1.39822065e-01   2.63503011e-02   6.76846867e-02  -9.89146118e-03\n",
      "    7.56585756e-02  -7.60478217e-02   2.43001408e-02   8.33677578e-02\n",
      "   -1.11647613e-01  -1.07357194e-01   1.21471131e-01   3.45338988e-02\n",
      "   -1.50493291e-01   7.27121284e-02  -7.23032379e-02   2.84145032e-03\n",
      "   -2.50384207e-01  -3.83316781e-02  -1.18872436e-01  -1.90617480e-01\n",
      "   -1.23741531e-01   8.10056235e-02   1.39282547e-01   7.06049733e-02\n",
      "    1.54505836e-01   9.91936767e-02  -7.79729015e-02   1.19530983e-01\n",
      "    1.91543516e-01  -1.05149519e-01  -6.38578135e-02  -4.50920843e-02\n",
      "   -9.26590337e-02   5.12054842e-02  -3.72833037e-02   1.76379181e-01\n",
      "   -3.87367795e-02  -5.95523966e-02   3.06710262e-02   5.09677489e-03\n",
      "    1.95929587e-01   8.32388358e-03   6.77483837e-03  -4.78484770e-02\n",
      "    1.12469289e-01   7.24317079e-02  -3.53199237e-02   4.32369556e-02\n",
      "   -3.52418530e-02  -8.00245593e-02  -1.66396811e-02  -9.31839034e-02\n",
      "   -6.28119214e-02  -9.54621109e-02  -3.31846146e-02   8.14655409e-02\n",
      "   -8.30920265e-02   6.00590503e-02  -1.46986639e-01   7.70949356e-02\n",
      "    1.43080711e-01  -1.11699361e-01  -8.46685274e-02   7.80059771e-02\n",
      "   -2.62189251e-03  -5.38078607e-03  -2.11106152e-02  -4.68213656e-02\n",
      "   -1.34563718e-01   1.96240398e-01  -1.85129898e-01  -8.83390609e-02\n",
      "    9.13696089e-02  -4.57266855e-02   9.59483964e-02   1.42264958e-01\n",
      "    1.11657998e-01  -8.71432673e-02  -1.05692976e-01  -6.01076315e-02\n",
      "   -6.97233434e-02  -2.99507683e-02  -3.08069185e-02  -1.70160758e-01\n",
      "   -2.79221580e-02  -5.34900745e-02  -1.21686508e-01  -3.91222771e-02\n",
      "   -1.17562335e-02  -3.68688087e-02   3.20884871e-02  -1.64072395e-02\n",
      "   -2.12600064e-02  -4.55873802e-02  -3.21047990e-03   1.51232184e-02\n",
      "    6.74386049e-02  -7.11608322e-02   6.82020753e-02  -2.25343685e-01\n",
      "   -1.58724032e-01   3.54782351e-02   1.30153722e-01   4.24581708e-02\n",
      "   -4.04752705e-02  -2.23087615e-02  -5.48827098e-02  -3.98662527e-02\n",
      "    8.03736073e-02  -7.46633678e-02  -8.99277543e-02  -4.63934644e-02\n",
      "    1.61431782e-02  -7.62058605e-02   1.73067565e-01  -1.02512049e-01\n",
      "    2.81329094e-02   1.88550426e-02   6.51175855e-02  -2.74920959e-03\n",
      "    1.65652923e-01  -5.13959860e-02  -3.31516984e-03   8.15395547e-02\n",
      "    1.84819406e-02  -2.29987294e-01   4.30680891e-02  -1.81449033e-02\n",
      "   -2.63437116e-02  -5.57121934e-02   3.17093883e-02   2.40094951e-02\n",
      "   -2.56889814e-02   8.06392434e-03  -9.95541617e-02  -2.07293597e-03\n",
      "   -5.83833361e-02   5.08218655e-03   4.70311458e-03  -2.15222594e-02\n",
      "    1.51916714e-01  -6.45860936e-02  -2.09641986e-02   1.71725430e-01\n",
      "    2.80971003e-02  -2.37504085e-01  -2.55473831e-02   8.34210413e-02\n",
      "   -3.71973064e-02   1.21585786e-01   4.00804719e-02   1.97615370e-01\n",
      "    4.53613812e-02   4.86045772e-02  -2.17608137e-01  -1.22914257e-01\n",
      "    4.89922679e-02   3.10822217e-02  -1.30260291e-01  -1.55736273e-02\n",
      "   -1.06795918e-01   1.38060926e-02   7.06156189e-02   1.28996703e-01\n",
      "    3.50224937e-02  -6.81955719e-02   9.22286427e-02  -1.96763319e-01\n",
      "   -7.08445131e-02   3.11407803e-01  -1.89415640e-01  -1.37946058e-01\n",
      "    7.62617404e-02   2.23421874e-02  -1.32210178e-01   1.16785546e-01\n",
      "   -2.35486599e-02  -7.24167854e-02   1.07590158e-02   4.64997384e-02\n",
      "   -2.23170208e-01  -5.99581876e-02  -1.30934018e-01  -9.03055622e-02\n",
      "   -1.91873293e-01  -5.40392841e-03   1.12529256e-01  -1.28736526e-01\n",
      "    7.77984000e-02  -1.44883948e-01   2.02058958e-02   2.69143729e-02\n",
      "    1.86574318e-01   9.66094739e-02  -6.85981542e-02  -6.82184033e-02\n",
      "   -2.09715108e-01   1.63942550e-02   1.32563192e-01   3.27073941e-02\n",
      "   -9.53639576e-02  -6.14267271e-04   9.90713914e-02   2.82025607e-01\n",
      "   -2.33603530e-02  -1.41942238e-01   5.24404615e-02  -4.17891814e-02\n",
      "   -2.70529402e-01   9.39676587e-02  -9.85822231e-04   4.94034537e-02\n",
      "   -2.25570685e-01  -9.54414596e-02   1.97725709e-02   1.27875593e-01\n",
      "    7.30860423e-02  -5.50842322e-02   1.64068924e-01  -1.21490730e-02\n",
      "   -1.18538904e-01  -1.62241131e-02   1.09725325e-01   1.13199057e-01\n",
      "    5.42621076e-02  -2.79429600e-01   1.86474470e-04   7.89488235e-02\n",
      "    1.14767123e-01  -6.74448664e-02  -6.38780057e-02  -2.78174204e-01\n",
      "   -5.87548854e-02  -3.83905308e-02  -1.91136740e-01   1.57259736e-01\n",
      "   -8.86836850e-03  -3.10097529e-02  -1.97132159e-01   2.26061793e-02\n",
      "   -1.67948803e-02   4.70238889e-02  -3.29660869e-02  -1.55821233e-01\n",
      "    4.73070094e-02   1.37931425e-01  -3.42636488e-02  -1.01615486e-01\n",
      "    4.00315114e-02  -1.49165714e-01  -1.67946140e-01  -1.29362053e-01\n",
      "   -7.12601262e-02   3.98452680e-02   1.08655084e-01   1.53866197e-01\n",
      "   -6.55067213e-02   2.74729614e-03   3.70052664e-02   8.33829841e-02\n",
      "   -6.00770365e-02   4.92581537e-04   6.54482770e-02  -1.03930478e-01\n",
      "   -9.45998103e-02  -2.24580553e-02   1.11148845e-01   1.06422978e-01\n",
      "    6.60912974e-02  -1.60705121e-01  -6.16977084e-03   7.79007641e-02\n",
      "    8.50440636e-02  -7.09916905e-02  -7.99379407e-02   1.22629895e-01\n",
      "   -7.12023494e-02  -1.53514814e-01   6.19090706e-02   6.27532569e-02\n",
      "    2.70557587e-01  -1.63556224e-01  -8.11236814e-02  -6.22748849e-02\n",
      "   -1.29462799e-01  -1.27297335e-02   4.56490306e-03  -1.50580120e-01\n",
      "   -1.50171581e-01   1.78309020e-01  -4.49631251e-02  -5.06539410e-03\n",
      "    2.20608086e-02   1.72342713e-01   1.07476140e-01   6.67928330e-02\n",
      "   -4.09068133e-02  -7.48575497e-02   2.40621312e-02   1.30871349e-01\n",
      "    6.77183027e-02   1.54005748e-01   2.03122845e-02   1.31022658e-01\n",
      "    6.32620239e-02   1.51258150e-01  -8.27122627e-02   3.28332913e-02\n",
      "   -1.73005384e-03  -3.37797702e-02   1.08222409e-01   5.95343733e-02\n",
      "   -6.30539026e-02   3.89066843e-02   2.06560349e-02  -2.93968243e-02\n",
      "   -8.46657309e-04  -1.98302709e-01  -1.45551186e-02   3.39268244e-02\n",
      "   -8.11581320e-03  -4.80988393e-02   5.04246009e-02  -2.97611203e-03\n",
      "    1.14940327e-01  -7.00749517e-02   1.85005544e-02   2.40532858e-02\n",
      "   -9.35912459e-02   1.27644067e-01  -3.43157488e-02  -1.05289761e-01\n",
      "   -1.96767493e-01  -1.35874056e-01  -1.19095476e-01   2.81534601e-02\n",
      "   -1.09182376e-01  -9.08337643e-02  -1.49411246e-01  -5.18955934e-02\n",
      "    1.12052545e-01   1.73933420e-02   3.65972940e-02  -5.21159552e-02\n",
      "   -6.39208008e-02   7.77422969e-02  -2.43754965e-02  -1.85724691e-01\n",
      "    5.14718904e-03   1.02420572e-01  -1.68199638e-01   1.03317517e-01\n",
      "   -2.21231025e-01   9.83714445e-02  -6.44147876e-02   1.03158361e-01\n",
      "    7.94999833e-03   9.76496421e-02  -3.21066434e-02  -1.75528108e-01\n",
      "    9.38996091e-02  -7.52808778e-02   1.30097090e-01   4.59498776e-03\n",
      "    1.95476628e-01   1.29274090e-01   1.67908376e-01   2.22718523e-01\n",
      "   -9.02926910e-02   1.61546599e-01  -5.70261141e-02   2.07490746e-03\n",
      "   -6.17535832e-02  -4.47383464e-02  -1.04927217e-01  -1.32425189e-01\n",
      "   -8.64151128e-02  -1.00819930e-02  -1.37609951e-01  -5.09437484e-02\n",
      "    5.00254562e-02  -6.62726275e-02  -1.15400133e-01  -2.12112110e-01\n",
      "    5.91144618e-03  -2.42913532e-01  -1.19691576e-01  -1.82070614e-02\n",
      "   -1.18605903e-01   8.16601017e-03  -4.69719552e-04   3.75933453e-02\n",
      "    7.00214900e-02   1.84544190e-03  -8.00556428e-02  -5.12204029e-02\n",
      "   -8.99060005e-02   1.34433415e-01  -2.05385883e-02   9.38803140e-02\n",
      "    1.43364841e-02  -1.94713525e-02   2.18053792e-02   7.49963137e-02\n",
      "    1.43328465e-01   2.39182539e-01   1.16147346e-01   1.40079213e-02\n",
      "   -4.07257360e-02   9.12630621e-02   5.61850320e-02  -9.82134409e-03\n",
      "   -2.38092025e-02  -1.01609948e-01   2.74461157e-01  -4.56961642e-02\n",
      "   -1.78022873e-03   1.44965659e-01   1.15690838e-03   4.85844240e-02\n",
      "   -8.14610045e-02   5.49887389e-02   2.28656333e-02   1.65233693e-01]]\n",
      "The derivatives of the cost with respect to the weights are:\n",
      "[[-0.05026157 -0.05095309 -0.01922676 ..., -0.01097281 -0.0675027\n",
      "  -0.01733678]\n",
      " [-0.05026157 -0.05095309 -0.01922676 ..., -0.01097281 -0.0675027\n",
      "  -0.01733678]\n",
      " [-0.05026157 -0.05095309 -0.01922676 ..., -0.01097281 -0.0675027\n",
      "  -0.01733678]\n",
      " ..., \n",
      " [-0.05026157 -0.05095309 -0.01922676 ..., -0.01097281 -0.0675027\n",
      "  -0.01733678]\n",
      " [-0.05026157 -0.05095309 -0.01922676 ..., -0.01097281 -0.0675027\n",
      "  -0.01733678]\n",
      " [-0.05026157 -0.05095309 -0.01922676 ..., -0.01097281 -0.0675027\n",
      "  -0.01733678]]\n",
      "The derivatives of the cost with respect to the biases are:\n",
      "[ 0.10052314  0.10190617  0.03845352  0.07902189 -0.93731516  0.09616092\n",
      "  0.32962493  0.02194563  0.13500539  0.03467356]\n"
     ]
    }
   ],
   "source": [
    "backward_pass(graph, debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinlu/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/kevinlu/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:18: RuntimeWarning: invalid value encountered in multiply\n",
      "/Users/kevinlu/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/kevinlu/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:45: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/kevinlu/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of validation set in epoch 1 is: nan\n",
      "0.0984166666667\n",
      "Loss of validation set in epoch 2 is: nan\n",
      "0.0984166666667\n",
      "Loss of validation set in epoch 3 is: nan\n",
      "0.0984166666667\n",
      "Loss of validation set in epoch 4 is: nan\n",
      "0.0984166666667\n",
      "Loss of validation set in epoch 5 is: nan\n",
      "0.0984166666667\n"
     ]
    }
   ],
   "source": [
    "# Now lets run the model\n",
    "from sklearn.utils import shuffle\n",
    "epochs = 5\n",
    "steps_per_epoch = 100\n",
    "batch_size = 128\n",
    "show_per_step = 1\n",
    "trainables = [W1, b1]\n",
    "accuracies = []\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    X_train_normalize, y_train_one_hot = shuffle(X_train_normalize, y_train_one_hot)\n",
    "    # First shuffle the data\n",
    "    for offset in range(0, X_train_normalize.shape[0], batch_size):\n",
    "       \n",
    "        # Define the ending points\n",
    "        end = offset + batch_size\n",
    "        batch_x, batch_y = X_train_normalize[offset:end], y_train_one_hot[offset:end]\n",
    "        \n",
    "        # Reset the values of X and y \n",
    "        X.value = batch_x\n",
    "        y.value = batch_y\n",
    "#         print(X.value)\n",
    "#         print(y.value)\n",
    "        \n",
    "        # Now run the forward and backward propagation\n",
    "#         if (end % (batch_size*100) == 0):\n",
    "#             print(\"offset is:\")\n",
    "#             print(offset)\n",
    "#             forward_pass(graph, debug = True) \n",
    "#             backward_pass(graph, debug = True)\n",
    "#             print(\"Loss is {0}\".format(graph[-1].value))\n",
    "#         else:\n",
    "#             forward_pass(graph)\n",
    "#             backward_pass(graph)\n",
    "#             print(\"Loss is{0}\".format(graph[-1].value))\n",
    "        \n",
    "        forward_pass(graph)\n",
    "        backward_pass(graph)\n",
    "        \n",
    "        # Update the weights of or biases and weights\n",
    "        sgd_update(trainables, learning_rate = 1e-3) \n",
    "        loss += graph[-1].value\n",
    "\n",
    "#     print(\"Epoch: {}, Loss {:.3f}\".format(i + 1, loss/steps_per_epoch))\n",
    "#     print(\"Average loss per sample is: {0}\".format(loss/(steps_per_epoch * batch_size)))\n",
    "    \n",
    "    # Use the validation set to see our accuracy\n",
    "    X.value = X_validation_normalize\n",
    "    y.value = y_validation_one_hot \n",
    "    forward_pass(graph)\n",
    "    accuracies.append(graph[-1].accuracy)\n",
    "    if (i%show_per_step == 0):\n",
    "        print(\"Loss of validation set in epoch {0} is: {1}\".format(i + 1, graph[-1].value)) \n",
    "        print(graph[-1].accuracy)\n",
    " \n",
    "\n",
    "# # Test it on the test set\n",
    "# X.value = X_test\n",
    "# y.value = y_test\n",
    "# forward_pass(graph)\n",
    "# print(\"Loss of test set is: {0}\".format(graph[-1].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
